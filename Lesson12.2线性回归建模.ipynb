{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 12.2 线性回归建模实验     \n",
    "\n",
    "## 一、深度学习建模流程\n",
    "\n",
    "![32](https://i.loli.net/2021/02/05/SlouvnpBxmJYZ4c.jpg)\n",
    "\n",
    "## 二、线性回归的手动实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机模块\n",
    "import random\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 自定义模块\n",
    "\n",
    "from torchLearning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.生成数据集\n",
    "\n",
    "&emsp;&emsp;利用此前的数据集生成函数，创建一个真实关系为$y = 2x_1-x_2+1$，且扰动项不是很大的回归类数据集。\n",
    "\n",
    "- y.numel() 返回y中元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mtensorGenReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "回归类数据集创建函数。\n",
      ":param num_examples: 创建数据集的数据量\n",
      ":param w: 包括截距的（如果存在）特征系数向量\n",
      ":param bias：是否需要截距\n",
      ":param delta：扰动项取值\n",
      ":param deg：方程次数\n",
      ":return: 生成的特征张和标签张量\n",
      "\u001b[1;31mFile:\u001b[0m      g:\\python学习资料2022年\\codetest\\pytorch&深度学习\\note_pytorch\\note_pytorch\\torchlearning.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "tensorGenReg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(420)   \n",
    "\n",
    "features, labels = tensorGenReg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.建模流程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Stage 1.模型选择  \n",
    "  围绕建模目标，我们可以构建一个只包含一层的神经网络进行建模。\n",
    "\n",
    "  <img src=\"https://i.loli.net/2021/02/05/UXeS3wZC7xiPN14.jpg\" alt=\"33\" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w):\n",
    "    return torch.mm(X,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 2.确定目标函数  \n",
    "  我们使用MSE作为损失函数，也就是目标函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat,y):\n",
    "    num_ = y.numel()\n",
    "    sse = torch.sum((y_hat.reshape(-1, 1) - y.reshape(-1, 1)) ** 2)\n",
    "    return sse / num_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 3.定义优化算法  \n",
    "采用小批量梯度下降进行求解，每一次迭代过程都是(参数-学习率*梯度)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params,lr):\n",
    "    params.data -= lr *params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**可微张量的in-place operation（对原对像修改操作）的相关讨论**\n",
    "\n",
    "(1).正常情况下，可微张量的in-place operation会导致系统无法区分叶节点和其他节点的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2., requires_grad = True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf\n",
    "\n",
    "# 开启可微之后，w的所有计算都会被纳入计算图中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = w * 2\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但如果在计算过程中，我们使用in-place operation，让新生成的值替换w原始值，则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17364\\2645349523.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2., requires_grad = True)\n",
    "w -= w * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中不允许叶节点使用in-place operation，根本原因是会造成叶节点和其他节点类型混乱。不过，虽然可微张量不允许in-place operation，但却可以通过其他方法进行对w进行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2., requires_grad = True)\n",
    "w = w * 2\n",
    "\n",
    "# 不过此时，w就不再是叶节点了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# 我们也无法通过反向传播求其导数\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "w = w * 2\n",
    "w.backward()       # w已经成为输出节点\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2).叶节点数值修改方法\n",
    "\n",
    "&emsp;&emsp;当然，如果出现了一定要修改叶节点的取值的情况，典型的如梯度下降过程中利用梯度值修改参数值时，可以使用此前介绍的暂停追踪的方法，如使用`with torch.no_grad()`语句或者`torch.detach()`方法，使得修改叶节点数值时暂停追踪，然后再生成新的叶节点带入计算，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2., requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————with torch.no_grad()————————————————————————————\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "\n",
    "# 利用with torch.no_grad()暂停追踪\n",
    "with torch.no_grad():\n",
    "    w -= w * 2\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————利用detach生成新变量————————————————————————————\n",
    "\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "\n",
    "# 利用detach生成新变量\n",
    "w.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w -= w * 2\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2., requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们介绍另一种方法，`.data`来返回可微张量的取值，从在避免在修改的过程中被追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(2., requires_grad = True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data # 查看张量的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w              # 但不改变张量本身可微性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.data -= w * 2  # 对其数值进行修改\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2., requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is_leaf       # 张量仍然是叶节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage.4 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 14.963247\n",
      "epoch 2, loss 47.127636\n",
      "epoch 3, loss 64.797066\n"
     ]
    }
   ],
   "source": [
    "# 设置随机数种子\n",
    "torch.manual_seed(420)    \n",
    "\n",
    "# 初始化核心参数\n",
    "batch_size = 10                                # 每一个小批的数量\n",
    "lr = 0.03                                      # 学习率\n",
    "num_epochs = 3                                 # 训练过程遍历几次数据\n",
    "w = torch.zeros(3, 1, requires_grad = True)    # 随机设置初始权重\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg                                   # 使用回归方程\n",
    "loss = squared_loss                            # MSE作为损失函数\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w),y)\n",
    "        l.backward()\n",
    "        sgd(w,lr)\n",
    "    train_l = loss(net(features,w),labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.4490],\n",
       "        [12.5513],\n",
       "        [10.9238],\n",
       "        [ 8.7683],\n",
       "        [21.7664],\n",
       "        [16.9534],\n",
       "        [ 8.1466],\n",
       "        [ 2.2295],\n",
       "        [ 7.1768],\n",
       "        [ 7.9883]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X,w)  # 返回预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0100],\n",
       "        [-5.9828],\n",
       "        [ 7.4010]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们也可以使用tensorboard记录上述迭代过程中loss的变化过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————tensorboard记录loss的变化————————————————————————————\n",
    "writer = SummaryWriter(log_dir='reg_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————tensorboard记录loss的变化————————————————————————————\n",
    "\n",
    "\n",
    "# 初始化核心参数\n",
    "batch_size = 10                                # 每一个小批的数量\n",
    "lr = 0.03                                      # 学习率\n",
    "num_epochs = 3                                 # 训练过程遍历几次数据\n",
    "w = torch.zeros(3, 1, requires_grad = True)    # 随机设置初始权重\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg                                   # 使用回归方程\n",
    "loss = squared_loss                            # 均方误差的一半作为损失函数\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w), y)\n",
    "        l.backward()\n",
    "        sgd(w, lr)\n",
    "    train_l = loss(net(features, w), labels)\n",
    "    # 记录训练误差\n",
    "    writer.add_scalar('mul1', train_l, epoch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、线性回归的快速实现\n",
    "\n",
    "- 调库实现 --> 定义类Class LR\n",
    "1. 定义模型 `Class LR(nn.Module):`\n",
    "   1. `def __init__()`\n",
    "   2. `def forward()`\n",
    "   3. 实例化类\n",
    "2. 定义损失函数`nn.MSELoss()`\n",
    "3. 定义优化方法`optim.SGD(模型.paramerters(), lr = 学习率)`\n",
    "4. 模型训练\n",
    "   1. ```\n",
    "      def fit(net, criterion, optimizer, batchdata, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for X, y in batchdata:\n",
    "                yhat = net.forward(X)\n",
    "                loss = criterion(yhat, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "      ```\n",
    "   2. `fit()` 训练模型\n",
    "- `list(模型.parameters())`\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义核心参数\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10                                # 每一个小批的数量\n",
    "lr = 0.03                                      # 学习率\n",
    "num_epochs = 3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子\n",
    "torch.manual_seed(420)   \n",
    "\n",
    "# 创建数据集\n",
    "features, labels = tensorGenReg()\n",
    "features = features[:, :-1]                                  # 剔除最后全是1的列\n",
    "data = TensorDataset(features, labels)                       # 数据封装\n",
    "batchData = DataLoader(data, batch_size = batch_size, shuffle = True)      # 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0070,  0.5044],\n",
       "        [ 0.6704, -0.3829],\n",
       "        [ 0.0302,  0.3826],\n",
       "        ...,\n",
       "        [-0.9164, -0.6087],\n",
       "        [ 0.7815,  1.2865],\n",
       "        [ 1.4819,  1.1390]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 1.定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, in_features=2, out_features=1):       # 定义模型的点线结构\n",
    "        super(LR, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):                                    # 定义模型的正向传播规则\n",
    "        out = self.linear(x)             \n",
    "        return out\n",
    "\n",
    "# 实例化模型\n",
    "LR_model = LR()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 2.定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 3.定义优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(LR_model.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stage 4.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, criterion, optimizer, batchdata, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in batchdata:\n",
    "            yhat = net.forward(X)\n",
    "            loss = criterion(yhat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        writer.add_scalar('loss', loss, global_step=epoch)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 当然，由于上述模型只有一层，因此也可以通过nn.Linear(2, 1)函数直接建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————训练模型————————————————————————————\n",
    "torch.manual_seed(420)   \n",
    "\n",
    "fit(net = LR_model, \n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer, \n",
    "    batchdata = batchData, \n",
    "    epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.0006, -1.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0008], requires_grad=True)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型参数\n",
    "list(LR_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算MSE\n",
    "criterion(LR_model(features), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据本身就是按照$y=2x_1-x_2+1$基本规律加上扰动项构建的，因此通过训练完成的参数可以看出模型效果较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们也可以通过add_graph方法，在writer中添加上述模型的记录图\n",
    "\n",
    "- `writer.add_graph(模型, (训练模型的数据,))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(LR_model, (features,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简单线性回归局限性\n",
    "\n",
    "此处我们进一步进行简单实验，当自变量和因变量满足最高次方为2次方的多项式函数关系时，或者扰动项增加时，简单线性回归误差将迅速增大。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1917, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————自变量和因变量的最高次为2次的情况————————————————————————————\n",
    "# Y = 2x1^2 - x2^2+1的情况\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(420)   \n",
    "\n",
    "# 创建数据集\n",
    "features, labels = tensorGenReg(deg=2)\n",
    "features = features[:, :-1]                                  # 剔除最后全是1的列\n",
    "data = TensorDataset(features, labels)\n",
    "batchData = DataLoader(data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# 模型实例化\n",
    "LR_model = LR()\n",
    "\n",
    "# 定义优化算法\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr = 0.03)\n",
    "\n",
    "# 模型训练\n",
    "fit(net = LR_model, \n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer, \n",
    "    batchdata = batchData, \n",
    "    epochs = num_epochs)\n",
    "\n",
    "# MSE结果查看\n",
    "criterion(LR_model(features), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0471, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————扰动项delta增加的情况————————————————————————————\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(420)   \n",
    "\n",
    "# 创建数据集\n",
    "features, labels = tensorGenReg(delta=2)\n",
    "features = features[:, :-1]                                  # 剔除最后全是1的列\n",
    "data = TensorDataset(features, labels)\n",
    "batchData = DataLoader(data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# 模型实例化\n",
    "LR_model = LR()\n",
    "\n",
    "# 定义优化算法\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr = 0.03)\n",
    "\n",
    "# 模型训练\n",
    "fit(net = LR_model, \n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer, \n",
    "    batchdata = batchData, \n",
    "    epochs = num_epochs)\n",
    "\n",
    "# MSE结果查看\n",
    "criterion(LR_model(features), labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3296471c9107718e6db2104fe506aa09d2e74f3fa2347eed57e489ba84541490"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
