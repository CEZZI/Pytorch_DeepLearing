{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.张量的线性代数运算\n",
    "\n",
    "即BLAS和LAPACK模块的相关运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、BLAS和LAPACK概览\n",
    "\n",
    "&emsp;&emsp;BLAS（Basic Linear Algeria Subprograms）和LAPACK（Linear Algeria Package）模块提供了完整的线性代数基本方法，包括：\n",
    "- 矩阵的形变及特殊矩阵的构造方法：包括矩阵的转置、对角矩阵的创建、单位矩阵的创建、上/下三角矩阵的创建等；\n",
    "- 矩阵的基本运算：包括矩阵乘法、向量内积、矩阵和向量的乘法等，当然，此处还包含了高维张量的基本运算，将着重探讨矩阵的基本运算拓展至三维张量中的基本方法；\n",
    "- 矩阵的线性代数运算：包括矩阵的迹、矩阵的秩、逆矩阵的求解、伴随矩阵和广义逆矩阵等；\n",
    "- 矩阵分解运算：特征分解、奇异值分解和SVD分解等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、矩阵的形变及特殊矩阵构造方法\n",
    "\n",
    "矩阵的形变方法其实也就是二维张量的形变方法\n",
    "\n",
    "**<center>Tensor矩阵运算</center>**\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.t(t)        | t转置| \n",
    "| torch.eye(n)       | 创建包含n个分量的单位矩阵 | \n",
    "| torch.diag(t1)        | 以t1中各元素，创建对角矩阵 | \n",
    "| torch.triu(t)        | 取矩阵t中的上三角矩阵 | \n",
    "| torch.tril(t)        | 取矩阵t中的下三角矩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建一个2*3的矩阵\n",
    "t1 = torch.arange(1, 7).reshape(2, 3).float()\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(t1)  # 转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 3, 0],\n",
       "        [0, 0, 0, 0, 4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(5)\n",
    "torch.diag(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 3, 0],\n",
       "        [0, 0, 0, 0, 0, 4],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线向上偏移一位\n",
    "torch.diag(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 2, 0, 0, 0],\n",
       "        [0, 0, 0, 3, 0, 0],\n",
       "        [0, 0, 0, 0, 4, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线向下偏移一位\n",
    "torch.diag(t, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [4., 5., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(9).reshape(3, 3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 4, 5],\n",
       "        [0, 0, 8]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取上三角矩阵\n",
    "torch.triu(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [0, 7, 8]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上三角矩阵向左下偏移一位\n",
    "torch.triu(t1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 0, 5],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上三角矩阵向右上偏移一位\n",
    "torch.triu(t1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、矩阵的基本运算\n",
    "\n",
    "**<center>矩阵的基本运算</center>**\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.dot(t1, t2)        | 计算t1、t2张量内积 | \n",
    "| torch.mm(t1, t2)        | 矩阵乘法 | \n",
    "| torch.mv(t1, t2)        | 矩阵乘向量 | \n",
    "| torch.bmm(t1, t2)        | 批量矩阵乘法 | \n",
    "| torch.addmm(t, t1, t2)        | 矩阵相乘后相加 | \n",
    "| torch.addbmm(t, t1, t2)        | 批量矩阵相乘后相加 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在PyTorch中，dot和vdot只能作用于一维张量，且对于数值型对象，二者计算结果并没有区别，两种函数只在进行复数运算时会有区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vdot(t, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mm：矩阵乘法\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(1, 7).reshape(2, 3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(1, 10).reshape(3, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [16, 25, 36]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对应位置元素相乘\n",
    "t1 * t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 36, 42],\n",
       "        [66, 81, 96]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "torch.mm(t1, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](https://i.loli.net/2021/01/14/gshVBOWM4QD2TiL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mv：矩阵和向量相乘      \n",
    "&emsp;&emsp;矩阵和向量相乘的过程我们可以看成是先将向量转化为列向量然后再相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met = torch.arange(1, 7).reshape(2, 3)\n",
    "met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = torch.arange(1, 4)\n",
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际执行向量和矩阵相乘的过程中，需要矩阵的列数和向量的元素个数相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(met, vec)\n",
    "\n",
    "# met中的每一个向量单独和vec做内积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14],\n",
       "        [32]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(met, vec.reshape(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bmm：批量矩阵相乘\n",
    "\n",
    "&emsp;&emsp;所谓批量矩阵相乘，指的是三维张量的矩阵乘法。根据此前对张量结构的理解，我们知道，三维张量就是一个包含了多个相同形状的矩阵的集合。\n",
    "\n",
    "例如，一个（3， 2， 2）的张量，本质上就是一个包含了3个2*2矩阵的张量。而三维张量的矩阵相乘，则是三维张量内部各对应位置的矩阵相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 3,  4]],\n",
       "\n",
       "        [[ 5,  6],\n",
       "         [ 7,  8]],\n",
       "\n",
       "        [[ 9, 10],\n",
       "         [11, 12]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.arange(1, 13).reshape(3, 2, 2)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15],\n",
       "         [16, 17, 18]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.arange(1, 19).reshape(3, 2, 3)\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9,  12,  15],\n",
       "         [ 19,  26,  33]],\n",
       "\n",
       "        [[ 95, 106, 117],\n",
       "         [129, 144, 159]],\n",
       "\n",
       "        [[277, 296, 315],\n",
       "         [335, 358, 381]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(t3, t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point:**     \n",
    "- 三维张量包含的矩阵个数需要相同；\n",
    "- 每个内部矩阵，需要满足矩阵乘法的条件，也就是左乘矩阵的行数要等于右乘矩阵的列数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addmm函数结构：`addmm(input, mat1, mat2, beta=1, alpha=1)`       \n",
    "输出结果：beta * input + alpha * (mat1 * mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 37, 44],\n",
       "        [66, 82, 98]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addmm(t, t1, t2)              # 先乘法后相加   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[300, 360, 420],\n",
       "        [660, 810, 960]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.addmm(t, t1, t2, beta = 0, alpha = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、矩阵的线性代数运算\n",
    "\n",
    "**<center>矩阵的线性代数运算</center>**\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.trace(A)       | 矩阵的迹 |\n",
    "| matrix_rank(A)       | 矩阵的秩 |\n",
    "| torch.det(A)         | 计算矩阵A的行列式 |  \n",
    "| torch.inverse(A)        | 矩阵求逆 | \n",
    "| torch.lstsq(A,B)        | 最小二乘法 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.矩阵的迹（trace）\n",
    "\n",
    "矩阵对角线元素之和\n",
    "- 当然，对于矩阵的迹来说，计算过程不需要是方阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()  \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，对于矩阵的迹来说，计算过程不需要是方阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(1, 7).reshape(2, 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.矩阵的秩(rank)\n",
    "\n",
    "矩阵的秩（rank），是指矩阵中行或列的极大线性无关数，且矩阵中行、列极大无关数总是相同的，任何矩阵的秩都是唯一值，满秩指的是方阵（行数和列数相同的矩阵）中行数、列数和秩相同，满秩矩阵有线性唯一解等重要特性，而其他矩阵也能通过求解秩来降维，同时，秩也是奇异值分解等运算中涉及到的重要概念。\n",
    "\n",
    "- matrix_rank计算矩阵的秩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 5).reshape(2, 2).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\py\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\LinearAlgebra.cpp:676.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2], [2, 4]]).float()\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.矩阵的行列式(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()     # 秩的计算要求浮点型张量\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.det(B)\n",
    "对于行列式的计算，要求二维张量必须是方阵，也就是行列数必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(1, 7).reshape(2, 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linalg.det: A must be batches of square matrices, but they are 3 by 2 matrices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-beff1455abd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: linalg.det: A must be batches of square matrices, but they are 3 by 2 matrices"
     ]
    }
   ],
   "source": [
    "torch.det(B)\n",
    "# B 不是方阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.线性方程组的矩阵表达形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 5).reshape(2, 2).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15117d60fc8>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制点图查看两个点的位置\n",
    "plt.plot(A[:,0], A[:, 1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**线性方程组的矩阵表示**\n",
    "$$Ax=B$$\n",
    "$$x=A^{-1}B$$\n",
    "\n",
    "> 当然，并非所有矩阵都有逆矩阵  \n",
    "> 满秩的方阵才有逆矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inverse函数：求解逆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [3., 1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 1], [3, 1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([2.0, 4])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求解x\n",
    "torch.mv(torch.inverse(A),B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即为$y = x+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、矩阵的分解\n",
    "\n",
    "常见的例如QR分解、LU分解、特征分解、SVD分解等等等等，虽然大多数情况下，矩阵分解都是在形式上将矩阵拆分成几种特殊矩阵的乘积\n",
    "\n",
    "而大多数情况下，矩阵分解都是分解成形如下述形式      \n",
    "$$ A = VUD$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.特征分解\n",
    "\n",
    "$$ A = Q\\Lambda Q^{-1}$$\n",
    "\n",
    "- $Q$和$Q^{-1}$互为逆矩阵，并且Q的列就是A的特征值所对应的特征向量\n",
    "- $\\Lambda$为矩阵A的特征值按照降序排列组成的**对角矩阵**\n",
    "\n",
    "- `torch.eig()`\n",
    "- `matrix_rank(B)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.eig函数：特征分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 10).reshape(3, 3).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\py\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.\n",
      "torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.\n",
      "L, _ = torch.eig(A)\n",
      "should be replaced with\n",
      "L_complex = torch.linalg.eigvals(A)\n",
      "and\n",
      "L, V = torch.eig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:3427.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[ 1.6117e+01,  0.0000e+00],\n",
       "        [-1.1168e+00,  0.0000e+00],\n",
       "        [ 2.9486e-07,  0.0000e+00]]),\n",
       "eigenvectors=tensor([[-0.2320, -0.7858,  0.4082],\n",
       "        [-0.5253, -0.0868, -0.8165],\n",
       "        [-0.8187,  0.6123,  0.4082]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(A, eigenvectors=True)                \n",
    " # 注，此处需要输入参数为True才会返回矩阵的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([1, 2, 2, 4]).reshape(2, 2).float()\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[0., 0.],\n",
       "        [5., 0.]]),\n",
       "eigenvectors=tensor([]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(B)          # 返回结果中只有一个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.],\n",
       "        [3., 6., 9.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.tensor([[1, 2, 3], [2, 4, 6], [3, 6, 9]]).float()\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[ 1.4000e+01,  0.0000e+00],\n",
       "        [ 6.2356e-08,  0.0000e+00],\n",
       "        [-2.8243e-07,  0.0000e+00]]),\n",
       "eigenvectors=tensor([]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(C)               \n",
    " # 只有一个特征的有效值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征值一般用于表示矩阵对应线性方程组解空间以及数据降维，当然，由于特征分解只能作用于方阵，而大多数实际情况下矩阵行列数未必相等，此时要进行类似的操作就需要采用和特征值分解思想类似的奇异值分解（SVD）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.奇异值分解（SVD）\n",
    "\n",
    "实际问题中大多数矩阵是以奇异矩阵形式，而不是方阵的形式出现的，奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为m×n的奇异矩阵A分解成三个部分 :      \n",
    "$$ A = U\\sum V^{T}$$    \n",
    "其中U、V是两个正交矩阵，其中的每一行（每一列）分别被称为左奇异向量和右奇异向量，他们和∑中对角线上的奇异值相对应，通常情况下我们只需要保留前k个奇异向量和奇异值即可，其中U是m×k矩阵，V是n×k矩阵，∑是k×k的方阵，从而达到减少存储空间的效果，即      \n",
    "$$ A_{m*n} = U_{m*m}\\sum_{m*n}V^{T}_{n*n}\\approx U_{m*k}\\sum_{k*k}V^{T}_{k*n}$$\n",
    "\n",
    "- `torch.svd(二维张量)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svd奇异值分解函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.],\n",
       "        [3., 6., 9.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[-2.6726e-01,  9.6362e-01, -3.7767e-08],\n",
       "        [-5.3452e-01, -1.4825e-01, -8.3205e-01],\n",
       "        [-8.0178e-01, -2.2237e-01,  5.5470e-01]]),\n",
       "S=tensor([1.4000e+01, 4.2751e-08, 1.6397e-15]),\n",
       "V=tensor([[-0.2673, -0.9636,  0.0000],\n",
       "        [-0.5345,  0.1482, -0.8321],\n",
       "        [-0.8018,  0.2224,  0.5547]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.svd(C)\n",
    "\n",
    "#矩阵S 主要的奇异值都集中在一列上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "CU, CS, CV = torch.svd(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证SVD分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4000e+01, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 4.2751e-08, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 1.6397e-15]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(CS)  # 根据奇异值生成对角矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.mm(CU, torch.diag(CS)), CV.t())\n",
    "\n",
    "# 还原了矩阵C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时我们可根据svd输出结果对C进行降维\n",
    "\n",
    "此时C可只保留第一列（后面的奇异值过小），即k=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2673],\n",
       "        [-0.5345],\n",
       "        [-0.8018]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1 = CU[:, 0].reshape(3, 1)         # U的第一列\n",
    "U1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.0000)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1 = CS[0]                           # C的第一个值\n",
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2673, -0.5345, -0.8018]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = CV[:, 0].reshape(1, 3)           # V的第一行\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm((U1 * C1), V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时输出的Cd矩阵已经和原矩阵C高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9c95e2fca121e63b81ef3e38ca658e3e94770f8b66e1c1e59a7460ee39b328d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
