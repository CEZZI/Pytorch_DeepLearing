{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 16 深度学习入门(下)\n",
    "\n",
    "## 一、架构对学习能力/鲁棒性的影响\n",
    "\n",
    "### 1.深度\n",
    "\n",
    "#### 1.1 困难和实践\n",
    "\n",
    "深度学习普遍认知： 更深的网络会展示出更强大的学习能力\n",
    "\n",
    "**深度**:卷积神经网络当中，带有权重的层的数量，也指全部层的数量。通常来说，深度越深，参数量越大，卷积网络的规模越大。\n",
    "\n",
    "- 输入图像的尺寸会限制我们可以选择的深度\n",
    "- PyTorch中，池化层的padding的参数值必须小于池化核尺寸的1/2，否则程序会报错\n",
    "- 卷积层的padding参数值虽然不受程序强制限制，但一般也会设置为至少是小于卷积核尺寸\n",
    "  - 当kernel=padding时，卷积层会增加不必要的计算，甚至会单纯增加噪音\n",
    "\n",
    "我们常见的特征图尺寸变化其实只有以下三种：    \n",
    "1. 宽高分别折半、或缩小更多：当步长为2以上，或者卷积核较大时，可以实现大幅度缩小特征图的尺寸。\n",
    "2. 不使用池化层，利用填充与卷积核尺寸的搭配，令特征图每经过一次卷积层，就缩小2个或4个像素，\n",
    "3. 利用填充与卷积核尺寸的搭配，令特征图每经过一次卷积层，尺寸不变，将缩小特征图的工作完全交给池化层来做\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**VGGNet:**\n",
    "- 核心思想就是使用多个连续且保持特征图尺寸不变的卷积层来增加深度，重复架构\n",
    "- 每重复几个卷积就会跟一个最大池化层，这种“n个卷积+池化”的结构在VGG中被称之为“块”（block）\n",
    "\n",
    "随着深度的加深，模型的学习能力大概率会增强，但深度与模型效果之间的关系不是线性的，可以增长的边际准确率是在递减的，准确率的变化会逐渐趋于平缓。对于复杂的数据集，神经网络的层数都达到了200+\n",
    "\n",
    "### 1.2 VGG16的复现\n",
    "\n",
    "VGG架构对于神经网络研究和使用都有重要的意义，它不仅简单、有效，而且非常适合用来做各种实验和测试。在我们已经详细复现AlexNet与LeNet5的基础上，VGG架构的代码就显得异常简单。在这里，我为大家提供输入为224x224的VGGNet16的详细架构和复现后的代码，大家可以参考。\n",
    "\n",
    "VGG16的架构用语言来表示则有：  \n",
    " \n",
    "**输入→（卷积x2+池化）x2 →（卷积x3+池化）x3 → FC层x3 →输出**\n",
    "\n",
    "1. 除了输出层外，所有的激活函数都是ReLU函数  \n",
    "2. 最后三个全连接层中的前两个全连接层前有Dropout层，p=0.5 \n",
    "3. 每一次卷积后面都要加上激活函数 ,每个线性层后面也需要激活函数\n",
    "\n",
    "![](https://gitee.com/bravojimoon/note-picture/raw/master/torch/4D4H1AZ5A%7DUHLI@%5B9$1A%60SW.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doctest import OutputChecker\n",
    "from turtle import forward\n",
    "from torch import conv2d, relu, softmax\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # block1\n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(64,64,3,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        # block2\n",
    "        self.conv3 = nn.Conv2d(64,128,kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128,128,kernel_size=3,padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # block3\n",
    "        self.conv5 = nn.Conv2d(128,256,kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # block4\n",
    "        self.conv8 = nn.Conv2d(256,512,kernel_size=3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
    "        self.conv10 = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # block5\n",
    "        self.conv11 = nn.Conv2d(512,512,3,padding=1)\n",
    "        self.conv12 = nn.Conv2d(512,512,3,padding=1)\n",
    "        self.conv13 = nn.Conv2d(512,512,3,padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2)\n",
    "\n",
    "        # fc\n",
    "        self.fc1 = nn.Linear(7*7*512,4096)\n",
    "        self.fc2 = nn.Linear(4096,4096)\n",
    "        self.fc3 = nn.Linear(4096,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool3(F.relu(self.conv7(x)))\n",
    "\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = self.pool4(F.relu(self.conv10(x)))\n",
    "\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = self.pool5(F.relu(self.conv13(x)))\n",
    "\n",
    "        x = x.view(-1,7*7*512)\n",
    "\n",
    "        x = F.relu(self.fc1(F.dropout(x,p= 0.5)))\n",
    "        x = F.relu(self.fc2(F.dropout(x,p= 0.5)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        output = F.softmax(x,dim=1)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG16                                    [10, 10]                  --\n",
       "├─Conv2d: 1-1                            [10, 64, 224, 224]        1,792\n",
       "├─Conv2d: 1-2                            [10, 64, 224, 224]        36,928\n",
       "├─MaxPool2d: 1-3                         [10, 64, 112, 112]        --\n",
       "├─Conv2d: 1-4                            [10, 128, 112, 112]       73,856\n",
       "├─Conv2d: 1-5                            [10, 128, 112, 112]       147,584\n",
       "├─MaxPool2d: 1-6                         [10, 128, 56, 56]         --\n",
       "├─Conv2d: 1-7                            [10, 256, 56, 56]         295,168\n",
       "├─Conv2d: 1-8                            [10, 256, 56, 56]         590,080\n",
       "├─Conv2d: 1-9                            [10, 256, 56, 56]         590,080\n",
       "├─MaxPool2d: 1-10                        [10, 256, 28, 28]         --\n",
       "├─Conv2d: 1-11                           [10, 512, 28, 28]         1,180,160\n",
       "├─Conv2d: 1-12                           [10, 512, 28, 28]         2,359,808\n",
       "├─Conv2d: 1-13                           [10, 512, 28, 28]         2,359,808\n",
       "├─MaxPool2d: 1-14                        [10, 512, 14, 14]         --\n",
       "├─Conv2d: 1-15                           [10, 512, 14, 14]         2,359,808\n",
       "├─Conv2d: 1-16                           [10, 512, 14, 14]         2,359,808\n",
       "├─Conv2d: 1-17                           [10, 512, 14, 14]         2,359,808\n",
       "├─MaxPool2d: 1-18                        [10, 512, 7, 7]           --\n",
       "├─Linear: 1-19                           [10, 4096]                102,764,544\n",
       "├─Linear: 1-20                           [10, 4096]                16,781,312\n",
       "├─Linear: 1-21                           [10, 10]                  40,970\n",
       "==========================================================================================\n",
       "Total params: 134,301,514\n",
       "Trainable params: 134,301,514\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 154.80\n",
       "==========================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 1084.46\n",
       "Params size (MB): 537.21\n",
       "Estimated Total Size (MB): 1627.68\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# 实例化\n",
    "vgg = VGG16()\n",
    "summary(vgg, input_size=(10, 3, 224, 224),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.感受野\n",
    "\n",
    "#### 2.1 认识感受野\n",
    "\n",
    "另一个常常被认为是与卷积神经网络的预测效果相关的因素是感受野的尺寸。\n",
    "\n",
    "- 稀疏交互：每一个神经元只受上一层的部分神经元的影响（相当与不是全连接的）  \n",
    "  \n",
    "- **感受野**  \n",
    "在深度卷积神经网络中，每个神经元节点都对应着输入图像上的某个区域，且该神经元仅受这个区域中的图像内容的影响，那么这个区域称之为神经元的感受野。\n",
    "\n",
    "#### 2.2 认识感受野的性质\n",
    "\n",
    "- 深度越大，感受野越大，池化层放大感受野的效率更高\n",
    "- 放大感受野，是否有极限\n",
    "  - 按理来说感受野有上限（图像的大小），但实际上没有上限，感受野越大越好！！\n",
    "    - 感受野大小比图像大小还大时，图像就会出现黑边，增加了噪音 -> 关注中心，周围模糊\n",
    "- 关注中心，周围模糊\n",
    "  - 位于图像中间的像素有更多可以影响最终特征图的“路径”，他们对最终特征图的影响更大，对卷积网络的分类造成的影响也会更大。\n",
    "\n",
    "\n",
    "#### 2.3 扩大感受野：膨胀卷积\n",
    "\n",
    "膨胀卷积就是在每个参与卷积计算的计算点上做“膨胀”操作，让计算点与计算点之间出现“空洞”，并跳过空洞进行计算的卷积方式。\n",
    "- 膨胀卷积不适合小物体的分割\n",
    "- 详细见PDF\n",
    "\n",
    "#### 2.4 感受野尺寸的计算\n",
    "\n",
    "对于第$l$个卷积层/池化层输出的特征图而言，假设卷积核尺寸为正方形、输入架构的图像尺寸也为正方形，则该图上任意一个神经元的感受野大小为：\n",
    "$$r_l = r_{l-1}+(k_l-1)*\\prod_{i=0}^{l-1}{s_i}$$\n",
    "\n",
    "- $r_{l-1}$为上一个卷积层/池化层的感受野的大小， \n",
    "- $k_l$这一层的卷积核/池化核的大小，\n",
    "- $s_i$是第$i$层的卷积/池化步长。  \n",
    "  \n",
    "不难发现，在这个公式的最后一部分，是$l$层之前所有对感受野有影响的卷积、池化层的步长的连乘结果，可见，感受野的大小只与卷积核的大小、各层的步长有关，感受野的大小与padding无关。\n",
    "\n",
    "- 全连接层不需要计算感受野\n",
    "- pytorch中自动进行感受野计算的包torch_receptive_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(size=(10,1,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————例子:计算LeNet和AlexNet感受野的尺寸————————————————————————————\n",
    "# 两个网络的架构在上一节已经实现过了\n",
    "\n",
    "\n",
    "# 每一层感受野的计算公式：\n",
    "# 这一层的感受野 = 上一层的感受野的尺寸 + (这一层核的尺寸-1 )*连乘(从最初一层到上一层的步长)\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)                        # l0=1  1+(5-1)*1 = 5\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)    # 5+(2-1)*(1*1) =  6 \n",
    "        #这里更换为maxpool，在原论文中为average pool\n",
    "        self.conv2 = nn.Conv2d(6,16,5)                       # 6 + (5-1)*(1*1*2) = 14  # 上一层的步长为2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)    # 14 +(2-1)*(1*1*2*1) = 16\n",
    "        self.fc1 = nn.Linear(16*5*5,120)                     # 线性层不影响感受野的大小\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1,16*5*5)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x),dim=1)\n",
    "        output = F.softmax(x.view(-1,16*5*5),dim=1)\n",
    "\n",
    " \n",
    "        \n",
    "data = torch.ones(size=(10,3,227,227)) #假设图像的尺寸为227x227\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #大卷积核、较大的步长、较多的通道\n",
    "        self.conv1 = nn.Conv2d(3,96,kernel_size=11, stride=4)    # 1 + (11-1)*1 = 11\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=2)        # 11 + (3-1)*1*4 = 19\n",
    "        \n",
    "        #卷积核、步长恢复正常大小，进一步扩大通道\n",
    "        self.conv2 = nn.Conv2d(96,256,kernel_size=5, padding=2)  # 19 +(5-1)*(1*4*2) = 51\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3,stride=2)        # 51 + (3-1)*(8) = 67\n",
    "        \n",
    "        #连续的卷积层，疯狂提取特征\n",
    "        self.conv3 = nn.Conv2d(256,384,kernel_size=3,padding=1) \n",
    "        self.conv4 = nn.Conv2d(384,384,kernel_size=3,padding=1)\n",
    "        self.conv5 = nn.Conv2d(384,256,kernel_size=3,padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.平移不变性\n",
    "\n",
    "**不变性**：如果我们能够识别出一张图像中的一个对象，那即便这个对象以完全不同的姿态呈现在别的图像中，我们依然可以识别出这个对象。对算法而言，不变性意味着在训练集上被成功识别的对象，即便以不同的姿态出现在测试集中，也应该能够被成功识别。\n",
    "\n",
    "**鲁棒性（robustness）**:\n",
    "- 衡量一个系统所具有的抵抗变化、使自身保持稳定的能力\n",
    "  - 在数据科学中，鲁棒性用于形容一个模型或算法在不同数据、不同环境下的表现是否稳定\n",
    "  - 特别地，一个过拟合的模型的鲁棒性一定是较低的，因为过拟合就意味着不能适用数据的变化\n",
    "- 大部分深层卷积网络的架构自带一定的“平移不变性”\n",
    "\n",
    "池化层只能解决平移不变性的问题，对其他不变性却无计可施，为了让卷积神经网络具备各类不变性，我们需要采取更强力的手段：数据增强（Data Augmentation）。\n",
    "\n",
    "**数据增强**是数据科学体系中常用的一种增加数据量的技术，它通过添加略微修改的现有数据、或从现有数据中重新合成新数据来增加数据量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、架构对参数量/计算量的影响\n",
    "\n",
    "深度学习模型天生就需要大量数据进行训练，因此每次训练中的参数量和计算量就格外关键，因此在设计卷积网络时，我们希望**相似预测效果下，参数量越少越好**。\n",
    "\n",
    "对于卷积神经网络中的任意元素（层或函数），有两种方式影响模型的参数量：  \n",
    "1. 这个层自带参数，其参数量与该层的超参数的输入有关  \n",
    "2. 这个层会影响feature map的尺寸，影响整体像素量和计算量，从而影响全连接层的输入  \n",
    "\n",
    "全连接层、bn通过第一种方式影响参数，而池化、padding、stride等操作则通过第二种方法影响参数，卷积层通过两种方式影响参数，dropout、激活函数等操作不影响参数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.卷积层\n",
    "\n",
    "#### 1.1 参数量计算\n",
    "\n",
    "一个卷积网络的卷积层究竟包含多少参数量，就是由卷积核的尺寸kernel_size、输入的通道数in_channels，输出的通道数out_channels（卷积核的数量）共同决定的。其参数量如下：\n",
    "$$N_{parameters}=(K_H*K_W*C_{in})*C_{out}+C_{out}$$\n",
    "\n",
    "其中，加号前面的部分是权重$w$的数量，加号之后的是偏置$b$的数量。\n",
    "- padding以及stride这些参数，不影响卷积层的所需要的参数量\n",
    "- 卷积核的尺寸都比较小，真正影响参数量的是输入的参数和输入的特征图的大小\n",
    "- `.numel()` torch中用与计数的函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————例子：计算参数————————————————————————————\n",
    "\n",
    "# 实例化 - 实例化的瞬间就会生成随机的参数\n",
    "conv1 = nn.Conv2d(3,6,3)   # 3*3*3*6+6\n",
    "conv2 = nn.Conv2d(6,4,3)   # 3*3*6*4 +4\n",
    "\n",
    "conv1.weight.numel()\n",
    "conv1.bias.numel()\n",
    "\n",
    "# numel() - tensor中计数的函数\n",
    "\n",
    "\n",
    "conv2.weight.numel()\n",
    "conv2.bias.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#——————————————————————————padding以及stride这些参数，不影响卷积层的所需要的参数量———————————————————————————\n",
    "conv3 = nn.Conv2d(4,16,5,stride=2,padding=1)  # 5*5*4*16+16 = 1616  = 1600 +16\n",
    "\n",
    "conv4 = nn.Conv2d(16,3,5,stride=3,padding=2)  # 5*5*16*3+3  \n",
    "# (5*5*16)*3 + 3\n",
    "\n",
    "conv3.weight.numel()\n",
    "conv3.bias.numel()\n",
    "\n",
    "conv4.weight.numel()\n",
    "conv4.bias.numel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 大尺寸卷积核vs小尺寸卷积核\n",
    "\n",
    "- 大尺寸卷积核的效果可由多个小尺寸卷积核累积得到，默认都是用小卷积核\n",
    "\n",
    "#### 1.3 1x1卷积核\n",
    "在众多的小卷积核中，小到极致的就是1x1尺寸的卷积核。\n",
    "- 加深CNN的深度\n",
    "- 用两个卷积层之间，用于调整输出的通道数，协助大幅度降低计算量和参数量，从而协助加深网络深度，**跨通道信息交互**\n",
    "  - 下面两种方法都使用1*1卷积核：\n",
    "  - 方法1.`conv(256,256,3)`\n",
    "    - 需要$3*3*256*256+256=590080$个参数,\n",
    "  - 方法2.利用`conv(256,32,1)->conv(32,32,3)->conv(32,256,1) `\n",
    "    - 需要$(256*32+32)+(3*3*32*32+32)+(32*256*1+256)=25920$个参数 \n",
    "  - 参数量大大的缩减\n",
    "\n",
    " \n",
    "- **瓶颈设计（bottleneck design）**:在核尺寸为1x1的2个卷积层之间包装其他卷积层的架构\n",
    "  - 瓶颈设计常用于多于100层以上的网络中\n",
    "\n",
    "1x1的卷积核上只有一个权重，每次进行卷积操作时，该权重会与原始图像中每个像素相乘，并得到特征图上的新像素，因此1x1卷积也被叫做“逐点卷积”（Pointwise Convolution）,这种计算方式和矩阵*常数一致\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 减少参数量：分组卷积与深度分离卷积\n",
    "\n",
    "- 参数`groups`设置分组数,一般为偶数；将输入的特征图和输出的特征图都分成g组\n",
    "- 分组卷积也是一种高效降低参数数量的形式,通过给输入特征图及输出特征图分组来消减连接数量的卷积方式\n",
    "- 分组数g一般为偶数，将输入的特征图和输出的特征图都分成g组\n",
    "- 分组的存在不影响偏置，偏置只与输出的特征图数量有关\n",
    "\n",
    "\n",
    "例如：\n",
    "  - 不分组: conv(4,8,3) \n",
    "    -  参数：$3*3*4*8 =288$\n",
    "  - 分组： conv(4,8,3,g=1) & conv(4,8,3,g=2) \n",
    "    - 参数: $3*3*2*4 + 3*3*2*4 =144$\n",
    "     - 4个输入8个输出分成两组，每一组就是2个输入和4个输出\n",
    "  - 参数数量减少50% \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————例子：分组卷积————————————————————————————\n",
    "conv1 = nn.Conv2d(4,8,3) #(3 * 3 * 4)*8 + 8 = 296\n",
    "conv1_ = nn.Conv2d(4,8,3,groups=2) # ((3 * 3 * 4)*8)/2 + 8 = 152\n",
    "\n",
    "#检查一下结果\n",
    "conv1.weight.numel()\n",
    "conv1.bias.numel()\n",
    "\n",
    "conv1_.weight.numel()\n",
    "conv1_.bias.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in_channels must be divisible by groups",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\3932790791.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 输入奇数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 直接报错：in_channels must be divisible by groups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 输入输出的数据 要能够被groups除尽\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mpadding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[0mdilation_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m         super(Conv2d, self).__init__(\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m             False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
      "\u001b[1;32mc:\\Users\\HP\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0min_channels\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'in_channels must be divisible by groups'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'out_channels must be divisible by groups'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in_channels must be divisible by groups"
     ]
    }
   ],
   "source": [
    "# 输入奇数\n",
    "conv2 = nn.Conv2d(4,8,3,groups=3)\n",
    "\n",
    "# 直接报错：in_channels must be divisible by groups\n",
    "# 输入输出的数据 要能够被groups除尽"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 深度卷积\n",
    "  - groups参数最大可以取到和$C_{in}$或$C_{out}$中较小的那个值一样大。\n",
    "  - **groups =$C_{in}$的分组卷积叫做“深度卷积”**（Depthwise Convolution）\n",
    "  - 比起普通卷积，参数量是原来的$\\frac{1}{C_{in}}$  \n",
    "\n",
    "\n",
    "- 深度卷积与1x1卷积核结合使用  \n",
    "我们首先进行深度卷积，产出一组特征图，然后再这组特征图的基础上执行1x1卷积，对特征图进行线性变换。两种卷积打包在一起成为一个block，这个block就叫做“深度可分离卷积”（Depthwise separable convolution）\n",
    "对于深度可分离卷积的一个block，若不考虑偏置，则整个block的参数量为：\n",
    "$$parameters = K_H*K_W*C_{out}^{depth}+C_{in}^{pair}*C_{out}^{pait}$$\n",
    "\n",
    "若原始卷积也不考虑偏置，则深度可分离卷积的参数比上原始卷积的参数的比例为：\n",
    "  $$ratio = \\frac{C_{out}^{depth}}{C_{in}^{depth}*C_{out}^{pair}}+\\frac{C_{in}^{pair}}{K_H*K_w*C_{in}^{depth}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 普通卷积： (ks^2*c_in)*c_in\n",
    "# 分组卷积: 1/g*(ks^2*C_in * C_out), 深度卷积就是g= C_in\n",
    "# 深度可分离卷积： ks^2*C_in_depth + C_in_pair* C_in_pair\n",
    "# 比例： 1/C_in_depth + C_out_pair /(ks^2*C_in_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————普通卷积————————————————————————————\n",
    "#与图上不同，在代码中我们令输出的特征图数量与输入的特征图数量不相等，用以区别。输出特征图数量=8。\n",
    "conv1 = nn.Conv2d(4,8,3, bias=False) # (3 * 3 * 4) * 8 = 288\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————深度可分离卷积————————————————————————————\n",
    "# 深度卷积 groups = C_in\n",
    "conv_depthwise = nn.Conv2d(4,8,3,groups=4,bias=False) # 1/4 * (3 * 3 * 4)*8 = 72\n",
    "\n",
    "# 逐点卷积 kernel_size = 1\n",
    "conv_pairwise = nn.Conv2d(8,8,1,bias=False) # 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4722222222222222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.4722222222222222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————验证比例公式————————————————————————————\n",
    "# 利用比例公式计算\n",
    "1/4 + 8/(3*3*4)\n",
    "\n",
    "# 提取参数计算比例\n",
    "(conv_depthwise.weight.numel()+conv_pairwise.weight.numel())/(conv1.weight.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.全连接层\n",
    "\n",
    "真正对CNN参数量“贡献”巨大的是全连接层。数据在进入全连接层时，需要将所有像素拉平，而全连接层中的一个像素点就对应着一个参数，因此全连接层所携带大量参数。为什么卷积网络里需要有全连接层呢？全连接层的作用主要有以下两个：\n",
    "\n",
    "**1. 作为分类器，实现对数据的分类**\n",
    "\n",
    "卷积层提供了一系列有意义且稳定的特征值，构成了一个与输入图像相比维数更少的特征空间，而全连接层负责学习这个空间上的（可能是非线性的）函数关系，并输出预测结果。\n",
    "\n",
    "**2、作为整合信息的工具，将特征图中的信息进行整合**\n",
    "\n",
    "基于上面的两个理由，我们一般都会在形似AlexNet或VGG的网络中包含全连接层。但一旦有可以替换全连接层、并不影响模型效果的手段，大家就会尝试将全连接层替代掉。\n",
    "\n",
    "对于CNN中的全连接层来说，在一个层上增加足够多的神经元，会比增加层效果更好。一般来说，CNN中的全连接层最多只有3-4层（包括输出层），过于多的层会增加计算的负担，还会将模型带入过拟合的深渊。对于小型网络，3层全连接层已是极限了。\n",
    "\n",
    "需要注意的是，在卷积层和全连接层的连接中，通常全连接的输出神经元个数不会少于输入的通道数。对于全连接层之间的连接，只要不是输出层，也很少出现输出神经元少于输入神经元的情况。\n",
    "\n",
    "\n",
    "#### 2.1 从卷积到全连接层\n",
    "\n",
    "决定全连接层参数数量的有两个因素：最后一个卷积层上的特征图所含的像素量，以及我们在全连接层之间设定的输出神经元个数。\n",
    "\n",
    "-  **如何找出最后一个池化层/卷积层上输出的特征图的尺寸呢？**\n",
    "一种简单的方法是，将Model中所有的线性层都注释掉，只留下卷积层，然后将model输入summary进行计算，但有更简单的方法，使用另一种构筑神经网路架构的方式：nn.Sequential。  \n",
    "\n",
    "`nn.Sequential`是一种非常简单的构筑神经网络的方式，它可以将“以序列方式从前往后运行的层”打包起来，组合成类似于机器学习中的管道（Pipeline）的结构.\n",
    "\n",
    "- 代码量明显较少，但是卷积层的架构不太明显\n",
    "- 使用`nn.Sequential`来调试卷积层架构，并不断查看感受野的变化。\n",
    "- `nn.Sequential`中无须写明激活函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 9, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————使用nn.Squential架构网络————————————————————————————\n",
    "\n",
    "data = torch.ones(size=(10,3,229,229))\n",
    "\n",
    "#不使用类，直接将需要串联的网络、函数等信息写在一个“序列”里\n",
    "#重现上面的4个卷积层、2个池化层的架构\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(3,6,3),\n",
    "                    nn.ReLU(inplace=True)  # 实行ReLU以后，立即生效\n",
    "                    ,nn.Conv2d(6,4,3)\n",
    "                    ,nn.ReLU(inplace=True)\n",
    "                    ,nn.MaxPool2d(2)\n",
    "                    ,nn.Conv2d(4,16,5,stride=2,padding=1)\n",
    "                    ,nn.ReLU(inplace=True)\n",
    "                    ,nn.Conv2d(16,3,5,stride=3,padding=2)\n",
    "                    ,nn.ReLU(inplace=True)\n",
    "                    ,nn.MaxPool2d(2)\n",
    "                    )\n",
    "\n",
    "\n",
    "#nn.Sequential组成的序列不是类，因此不需要实例化，可以直接输入数据\n",
    "net(data).shape\n",
    "\n",
    "# 3个特征图每个是9*9\n",
    "\n",
    "\n",
    "# 调试神经网络的时候使用，不需要专门定义一个类\n",
    "# 不需要写入线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 7, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————利用nn.Squential查看输入到全连接层的输入数————————————————————————————\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(3,64,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(64,64,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(64,128,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(128,128,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(128,256,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(256,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),        \n",
    "                                        )\n",
    "\n",
    "data = torch.ones(size=(10,3,224,224))\n",
    "\n",
    "net(data).shape\n",
    "\n",
    "# 512个特征图，尺寸为7*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VGG16架构\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/7.png?versionId=CAEQFRiBgMD2zKyfxxciIGZhMDQ0Y2UyYTA5ZjQ1NjhhMWNjNDQ1Njg3YTFiODZh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————VGG16为例，使用nn.Sequential的架构————————————————————————————\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 特征提取\n",
    "        self.features_ = nn.Sequential(nn.Conv2d(3,64,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(64,64,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(64,128,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(128,128,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(128,256,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(256,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),\n",
    "\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True),\n",
    "                                       nn.MaxPool2d(2),        \n",
    "                                        )\n",
    "        # 分类器\n",
    "        self.clf_ = nn.Sequential(nn.Dropout(p=0.5),\n",
    "                                nn.Linear(512*7*7,4096),nn.ReLU(inplace=True),  # 512*7*7 在上一个cell中利用nn.Squential计算的\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(4096,4096,nn.ReLU(inplace=True)),\n",
    "                                nn.Linear(4096,1000),nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.features_(x)  # 用于特征提取的架构提取特征\n",
    "        x = x.view(-1,512*7*7)  # 调整数据结构，拉平函数\n",
    "        output = self.clf_(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16()  # 实例化定义好的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG16                                    [10, 1000]                --\n",
       "├─Sequential: 1-1                        [10, 512, 7, 7]           --\n",
       "│    └─Conv2d: 2-1                       [10, 64, 224, 224]        1,792\n",
       "│    └─ReLU: 2-2                         [10, 64, 224, 224]        --\n",
       "│    └─Conv2d: 2-3                       [10, 64, 224, 224]        36,928\n",
       "│    └─ReLU: 2-4                         [10, 64, 224, 224]        --\n",
       "│    └─MaxPool2d: 2-5                    [10, 64, 112, 112]        --\n",
       "│    └─Conv2d: 2-6                       [10, 128, 112, 112]       73,856\n",
       "│    └─ReLU: 2-7                         [10, 128, 112, 112]       --\n",
       "│    └─Conv2d: 2-8                       [10, 128, 112, 112]       147,584\n",
       "│    └─ReLU: 2-9                         [10, 128, 112, 112]       --\n",
       "│    └─MaxPool2d: 2-10                   [10, 128, 56, 56]         --\n",
       "│    └─Conv2d: 2-11                      [10, 256, 56, 56]         295,168\n",
       "│    └─ReLU: 2-12                        [10, 256, 56, 56]         --\n",
       "│    └─Conv2d: 2-13                      [10, 256, 56, 56]         590,080\n",
       "│    └─ReLU: 2-14                        [10, 256, 56, 56]         --\n",
       "│    └─Conv2d: 2-15                      [10, 256, 56, 56]         590,080\n",
       "│    └─ReLU: 2-16                        [10, 256, 56, 56]         --\n",
       "│    └─MaxPool2d: 2-17                   [10, 256, 28, 28]         --\n",
       "│    └─Conv2d: 2-18                      [10, 512, 28, 28]         1,180,160\n",
       "│    └─ReLU: 2-19                        [10, 512, 28, 28]         --\n",
       "│    └─Conv2d: 2-20                      [10, 512, 28, 28]         2,359,808\n",
       "│    └─ReLU: 2-21                        [10, 512, 28, 28]         --\n",
       "│    └─Conv2d: 2-22                      [10, 512, 28, 28]         2,359,808\n",
       "│    └─ReLU: 2-23                        [10, 512, 28, 28]         --\n",
       "│    └─MaxPool2d: 2-24                   [10, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-25                      [10, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-26                        [10, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-27                      [10, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-28                        [10, 512, 14, 14]         --\n",
       "│    └─Conv2d: 2-29                      [10, 512, 14, 14]         2,359,808\n",
       "│    └─ReLU: 2-30                        [10, 512, 14, 14]         --\n",
       "│    └─MaxPool2d: 2-31                   [10, 512, 7, 7]           --\n",
       "├─Sequential: 1-2                        [10, 1000]                --\n",
       "│    └─Dropout: 2-32                     [10, 25088]               --\n",
       "│    └─Linear: 2-33                      [10, 4096]                102,764,544\n",
       "│    └─ReLU: 2-34                        [10, 4096]                --\n",
       "│    └─Dropout: 2-35                     [10, 4096]                --\n",
       "│    └─Linear: 2-36                      [10, 4096]                16,781,312\n",
       "│    └─Linear: 2-37                      [10, 1000]                4,097,000\n",
       "│    └─Softmax: 2-38                     [10, 1000]                --\n",
       "==========================================================================================\n",
       "Total params: 138,357,544\n",
       "Trainable params: 138,357,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 154.84\n",
       "==========================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 1084.54\n",
       "Params size (MB): 553.43\n",
       "Estimated Total Size (MB): 1643.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from receptive_field import receptive_field\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(vgg,input_size=(10,3,224,224),device='cpu')  # 会自动调用GPU\n",
    "\n",
    "# 没有全部显示完全\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 代替全连接层：1x1卷积核与全局平均池化（GAP）\n",
    "\n",
    "- 是否能用1x1卷积核来进行替代全连接层?\n",
    "  - 可以，但是使用效果微乎及微\n",
    "  - 参数量没有减少...\n",
    "  - 最大的好处就是解放了输入层对图像尺寸的限制...\n",
    "    - 在拉平进入到输入层中，要求具体的输入数\n",
    "    - 如果全部是卷积层就不需要拉平\n",
    "\n",
    "\n",
    "对于卷积层来说，只要让特征图的尺寸为1x1，再让卷积核的尺寸也为1x1，就可以实现和普通全连接层一模一样的计算了。\n",
    "- 不包含全连接层，只有卷积层和池化层的卷积网络被叫做全卷积网络（fullyconvolutional network，FCN）。\n",
    "\n",
    "\n",
    "- NiN网络中用全局平均池化GAP代替了全连接层\n",
    "  - GAP层的本质是池化层，它使用池化方式是平均池化，它的职责就是将上一层传入的无论多少特征图都转化成(n_class,1, 1)结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.NiN网络的复现\n",
    "\n",
    "- NiN网络使用1x1卷积层(实际上是GAP层)代替的全连接层\n",
    "\n",
    "我们就使用nn.Sequential来打包实现一下NiN网络。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/58.png?versionId=CAEQFRiBgIC0zIWfxxciIGE0ZjkyMzI1ZTE3MzQ3MmQ5NDhiZWFlN2UyMmFiYTQ0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入数据的尺寸\n",
    "data = torch.ones(10,3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import dropout\n",
    "\n",
    "# 为什么要把特征图的尺寸 先放大在放下？\n",
    "\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3,192,5,padding=2),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192,160,1),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(160,96,1),nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),   # 步长小于核尺寸 -> 重叠池化\n",
    "            nn.Dropout(p=0.25)  # 未明确给出，但是卷积层的的p值一般在0.25以下，线性层的p值给的比较大0.5 \n",
    "        )\n",
    "        self.block2 =nn.Sequential(\n",
    "            nn.Conv2d(96,192,5,padding=2),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192,192,1),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192,192,1),nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.block3 =nn.Sequential(\n",
    "            nn.Conv2d(192,192,3,padding=1),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192,192,1),nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192,10,1),nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(7,stride=1),   # 全局平均池化\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.block3(self.block2(self.block1(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化 \n",
    "net = NiN()\n",
    "net(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NiN                                      [10, 10, 1, 1]            --\n",
       "├─Sequential: 1-1                        [10, 96, 15, 15]          --\n",
       "│    └─Conv2d: 2-1                       [10, 192, 32, 32]         14,592\n",
       "│    └─ReLU: 2-2                         [10, 192, 32, 32]         --\n",
       "│    └─Conv2d: 2-3                       [10, 160, 32, 32]         30,880\n",
       "│    └─ReLU: 2-4                         [10, 160, 32, 32]         --\n",
       "│    └─Conv2d: 2-5                       [10, 96, 32, 32]          15,456\n",
       "│    └─ReLU: 2-6                         [10, 96, 32, 32]          --\n",
       "│    └─MaxPool2d: 2-7                    [10, 96, 15, 15]          --\n",
       "│    └─Dropout: 2-8                      [10, 96, 15, 15]          --\n",
       "├─Sequential: 1-2                        [10, 192, 7, 7]           --\n",
       "│    └─Conv2d: 2-9                       [10, 192, 15, 15]         460,992\n",
       "│    └─ReLU: 2-10                        [10, 192, 15, 15]         --\n",
       "│    └─Conv2d: 2-11                      [10, 192, 15, 15]         37,056\n",
       "│    └─ReLU: 2-12                        [10, 192, 15, 15]         --\n",
       "│    └─Conv2d: 2-13                      [10, 192, 15, 15]         37,056\n",
       "│    └─ReLU: 2-14                        [10, 192, 15, 15]         --\n",
       "│    └─MaxPool2d: 2-15                   [10, 192, 7, 7]           --\n",
       "│    └─Dropout: 2-16                     [10, 192, 7, 7]           --\n",
       "├─Sequential: 1-3                        [10, 10, 1, 1]            --\n",
       "│    └─Conv2d: 2-17                      [10, 192, 7, 7]           331,968\n",
       "│    └─ReLU: 2-18                        [10, 192, 7, 7]           --\n",
       "│    └─Conv2d: 2-19                      [10, 192, 7, 7]           37,056\n",
       "│    └─ReLU: 2-20                        [10, 192, 7, 7]           --\n",
       "│    └─Conv2d: 2-21                      [10, 10, 7, 7]            1,930\n",
       "│    └─ReLU: 2-22                        [10, 10, 7, 7]            --\n",
       "│    └─AvgPool2d: 2-23                   [10, 10, 1, 1]            --\n",
       "│    └─Softmax: 2-24                     [10, 10, 1, 1]            --\n",
       "==========================================================================================\n",
       "Total params: 966,986\n",
       "Trainable params: 966,986\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.12\n",
       "Forward/backward pass size (MB): 48.61\n",
       "Params size (MB): 3.87\n",
       "Estimated Total Size (MB): 52.60\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net,input_size=(10,3,32,32))\n",
    "\n",
    "# 在Sequential中分开定义以后，summary也分层展示\n",
    "\n",
    "#作为9层卷积层、最大特征图数目达到192的网络，NiN的参数量在百万之下，可以说都是归功于没有使用全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为9层卷积层、最大特征图数目达到192的网络，NiN的参数量在百万之下，可以说都是归功于没有使用全连接层。不过，1x1卷积层所带来的参数量也不少，因此NiN可以说是在各方面都中规中矩的网络。\n",
    "\n",
    "受到NiN网络启发而诞生的GoogLeNet以及ResNet都使用了1x1卷积层，并且在各种消减参数的操作下使网络变得更加深。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、前沿网络state-of-the-art models\n",
    "\n",
    "在深度学习的领域，最前沿、最先进的架构被称为state-of-the-art models，简写为SOTA，我将其翻译为“前沿网络”。\n",
    "\n",
    "### 1.GoogLeNet(Inveption V1)\n",
    "\n",
    "#### 1.1 动机与思路\n",
    "\n",
    "VGG的参数过,各层之间的链接过于“稠密”（Dense），计算量过大，并且很容易过拟合。为了解决这个问题，我们之前已经提出了多种方法，其中最主流的是：  \n",
    "1. 使用我们在上一节中提出的分组卷积、舍弃全连接层等用来消减参数量的操作，让神经元与神经元之间、或特征图与特征图之间的连接数变少，从而让网络整体变得“稀疏”\n",
    "2. 引入随机的稀疏性。例如，使用类似于Dropout的方式来随机地让特征矩阵或权重矩阵中的部分数据为0\n",
    "3. 引入GPU进行计算\n",
    "\n",
    "在2014年之前，以上操作就是我们目前为止接触的所有架构在减少参数量、防止过拟合上做出的努力。其中NiN主要使用方法1，AlexNet和VGG主要使用方法2和3，但这些方法其实都存在一定的问题：\n",
    "\n",
    "首先，**分组卷积等操作虽然能够有效减少参数量，却也会让架构的学习水平变得不稳定**。在神经网络由稠密变得稀疏（Sparse）的过程中，网络的学习能力会波动甚至会下降。\n",
    "\n",
    "其次，**随机的稀疏性与GPU计算之间其实是存在巨大矛盾的**。现代硬件不擅长处理在随机或非均匀稀疏的数据上的计算，并且这种不擅长在矩阵计算上表现得尤其明显。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- GoogLeNet团队的思路是：  \n",
    "使用普通卷积、池化层这些稠密元素组成的块去无限逼近一个稀疏架构，从而构造一种参数量与稀疏网络相似的稠密网络。这种思路的核心不是通过减少连接、减少扫描次数等“制造空隙”的方式来降低稠密网络的参数量，而是直接在架构设计上找出一种参数量非常少的稠密网络。\n",
    "\n",
    "基于这样的基本理念，GoogLeNet团队使用了一个复杂的网络架构构造算法，并让算法向着“使用稠密成分逼近稀疏架构”的方向进行训练，产出了数个可能有效的密集架构。在进行了大量的实验之后，他们选出了学习能力最强的密集架构及其相关参数，这个架构就是Inception块和GoogLeNet。\n",
    "\n",
    "\n",
    "#### 1.2 Inception V1\n",
    "\n",
    "**Inception**块使用了卷积层、池化层并联的方式。在一个Inception块中存在4条线路，每条线路可以被叫做一个分枝（branch）：\n",
    "1. 第一条线路上只有一个1x1卷积层，只负责降低通道数\n",
    "2. 第二条路线由一个1x1卷积层和一个3x3卷积层组成，本质上是希望使用3x3卷积核进行特征提取，但先使用1x1卷积核降低通\n",
    "道数以此来降低参数量和计算量（降低模型的复杂度）\n",
    "3. 第三条线路由一个1x1卷积层和一个5x5卷积层组成，其基本思路与第二条线路一致\n",
    "4. 最后一条线路由一个3x3池化层和一个1x1卷积层组成，将池化也当做一种特征提取的方式,并在池化后使用1x1卷积层来降低通道数。\n",
    "5. 不难注意到，所有的线路都使了巧妙的参数组合，让特征图的尺寸保持不变，因此在四条线路分别输出结果之后，Inception块将四种\n",
    "方式生成的特征图拼接在一起，形成一组完整的特征图，\n",
    "\n",
    "![](https://gitee.com/bravojimoon/note-picture/raw/master/torch/%5D@1BCKVD9X$1%5DI@OQ91RCC9.png)\n",
    "\n",
    "优势：\n",
    "- 首先，同时使用多种卷积核可以确保各种类型和层次的信息都被提取出来。\n",
    "- 其次，并联的卷积池化层计算效率更高。\n",
    "- 大量使用1x1卷积层来整合信息，既实现了“聚类信息”又实现了大规模降低参数量，让特征图数量实现了前所未有的增长。  \n",
    "- Softmax作为主分类器，还有两个辅助分类器\n",
    "- GoogLeNet集成了两个浅层网络和一个深层网络的结果来进行学习和判断，在一个架构中间增加集成的思想\n",
    "\n",
    "  **辅助分类器**   \n",
    "  \n",
    "GoogLeNet还使用了“辅助分类器”（auxiliary classifier）以提升模型的性能。辅助分类器是除了主体架构中的softmax分类器之外，另外存在的两个分类器。在整体架构中，这两个分类器的输入分别是inception4a和inception4d的输出结果\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/辅助分类器.PNG?versionId=CAEQFRiBgIC7w62XyhciIGNkN2E5ZGRiMjlmZTQxYzY5YmY1ODA3YWI0YjYyNDYx)\n",
    "\n",
    "Google团队在迭代中加重中层inception输出的特征的权重，就可能将模型引导向更好的反向。因此，他们将位于中间的inceptions的结果使用辅助分类器导出，并让两个辅助分类器和最终的分类器一共输出三个softmax结果、依次计算三个损失函数的值，并将三个损失加权平均得到最终的损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 GoogLeNet的复现\n",
    "\n",
    "GoogLeNet是我们遇见的第一个串联元素中含有更多复杂成分的网络，因此我们需要先单独定义几个单独的元素，之后才能够使用我们熟悉的建立类的方式来复现架构。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义：基础卷积层\n",
    "\n",
    "首先，能够在主体网络中省略掉所有的激活函数，我们需要定义新的基础卷积层。这个卷积层是包含激活函数以及BN层的卷积层。这样定义能够帮助我们大幅度减少最后在整合好的GoogLeNet中会出现的ReLU函数以及BN层。\n",
    "\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/GoogleNet.PNG?versionId=CAEQFRiBgIDNw62XyhciIDA0ZDViOWM5YjYyNDRlZmJiMmMzNWVjOTZlNjk5YmMw)\n",
    "\n",
    "上图是GoogLeNet的主体架构。Inception内部是稠密部件的并联，而整个GoogLeNet则是数个Inception块与传统卷积结构的串联。\n",
    "\n",
    "- 深度学习中的神经网络bias一般为False\n",
    "- nn.Sequential中需要写明激活函数\n",
    "- 利用self.架构神经网络中不需要写激活函数，只需要在forward函数中定义数据的流向就行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv + BN + ReLU --> basicConv\n",
    "# Incepetion\n",
    "# AUXclcf\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicConv2d(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————定义：基础的卷积层 - 包含激活函数和BN层————————————————————————————\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    #在这里我们要改掉原来会输入参数默认值的习惯，而使用定义类型的方式，同时将**kwargs也放到init中继承\n",
    "    #**kwargs代表了“所需要的全部参数”，由于现在的架构变得复杂，我们不太可能将每个需要用的参数都写在定义中\n",
    "    #因此，我们继承**kwargs来获得所需类的全部参数\n",
    "    def __init__(self,in_channels:int,out_channels:int,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels,out_channels,bias=False,**kwargs),\n",
    "                                nn.BatchNorm2d(out_channels),\n",
    "                                nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output = self.conv(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "#测试\n",
    "BasicConv2d(2,10,kernel_size=3) #这里的输入数据是随意输入的，只是为了我们所写的类能够跑通\n",
    "\n",
    "# 试试看如果不写**kwargs，会发生什么？ - 报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义：Inception块\n",
    "  \n",
    "由于Inception块中是并联的结构，存在4个branchs，所以我们不能使用nn.Sequential进行打包，而是要使用原始的self.的形式。在Inception块中，所有卷积、池化层的输入、输出以及核大小都需要我们进行输入。Inception结构如下：\n",
    "\n",
    "![](https://gitee.com/bravojimoon/note-picture/raw/master/torch/%5D@1BCKVD9X$1%5DI@OQ91RCC9.png)\n",
    "\n",
    "- 4个分支最大不同就是输出的不同,通过参数传入\n",
    "- 3x3 reduce和5x5 reduce就是指inception块中3x3和5x5卷积层之前的1x1卷积层的输出量\n",
    "- pool proj中写的数字实际上是池化层后的1x1卷积层的输出量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————定义Inception块————————————————————————————\n",
    "class Inception(nn.Module):\n",
    "    \"\"\"\n",
    "    所有的数据都要通过这4个分支\n",
    "    \"\"\"\n",
    "    def __init__(self\n",
    "                ,in_channels :int\n",
    "                ,ch1x1 : int\n",
    "                ,ch3x3red : int\n",
    "                ,ch3x3 : int\n",
    "                ,ch5x5red : int\n",
    "                ,ch5x5 : int\n",
    "                ,pool_proj) -> None:   # 6个参数 属于6个卷积层\n",
    "        super().__init__()\n",
    "        \n",
    "        #  branch1 : 1x1\n",
    "        self.branch1 = BasicConv2d(in_channels,ch1x1,kernel_size = 1)\n",
    "        \n",
    "        # brach2 : 1x1 + 3x3\n",
    "        self.branch2 = nn.Sequential(BasicConv2d(in_channels,ch3x3red,kernel_size=1),\n",
    "                                     BasicConv2d(ch3x3red,ch3x3,kernel_size=3,padding=1))\n",
    "        \n",
    "        # 1x1+5x5\n",
    "        self.branch3 = nn.Sequential(BasicConv2d(in_channels,ch5x5red,kernel_size=1),\n",
    "                                     BasicConv2d(ch5x5red,ch5x5,kernel_size=5,padding=2))\n",
    "        \n",
    "        # pool + 1x1\n",
    "        self.branch4 = nn.Sequential(nn.MaxPool2d(kernel_size=3,stride=1,padding=1,ceil_mode=True),  # 池化层不改变数据的尺寸\n",
    "                                     BasicConv2d(in_channels,pool_proj,kernel_size=1))\n",
    "    \n",
    "    # 数据流是并联 - 最后合并在一起\n",
    "    def forward(self,x):\n",
    "        branch1 = self.branch1(x)  # 28x28,ch1x1\n",
    "        branch2 = self.branch2(x)  # 28x28,ch3x3\n",
    "        branch3 = self.branch3(x)  # 28x28,ch5x5\n",
    "        branch4 = self.branch4(x)  # 28x28,ch5x5\n",
    "        outputs = [branch1, branch2, branch3, branch4]   #  (28x28,ch1x1+ch3x3+ch5x5+ch5x5)\n",
    "        return torch.cat(outputs,1)  # 水平方向上合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "\n",
    "data = torch.ones(10,192,28,28)\n",
    "\n",
    "in3a = Inception(192,64,96,128,16,32,32) #这是inception3a的参数\n",
    "\n",
    "in3a(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义：辅助分类器\n",
    "  \n",
    "接下来，还需要单独定义的是辅助分类器（Auxiliary Classifier）的类。辅助分类器的结构其实与我们之前所写的传统卷积网络很相似，因此我们可以使用nn.Sequential来进行打包，并将分类器分成.features_和.clf_两部分来进行构建：\n",
    "\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/辅助分类器.PNG?versionId=CAEQFRiBgIC7w62XyhciIGNkN2E5ZGRiMjlmZTQxYzY5YmY1ODA3YWI0YjYyNDYx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxClf(nn.Module):\n",
    "    def __init__(self,in_channels:int,num_classes:int,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_ = nn.Sequential(nn.AvgPool2d(kernel_size=5,stride=3),\n",
    "                                    # nn.Conv2d(in_channels,128,1),\n",
    "                                    # nn.ReLU(inplace=True) nn.Sequential中须写明激活函数\n",
    "                                    # 为什么此处不写？ ---> 因此这里是BasicConv2d() 已经包含了激活函数\n",
    "                                    BasicConv2d(in_channels,128,kernel_size=1))\n",
    "        self.clf_ = nn.Sequential(nn.Linear(4*4*128,1024),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Dropout(p=0.7),\n",
    "                                  nn.Linear(1024,num_classes),  # 不一定要输出1000个分类\n",
    "                                  # nn.Softmax(dim=1)   假设使用的损失函数为交叉熵函数\n",
    "                                    )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.feature_(x)\n",
    "        x = x.view(-1,128*4*4)\n",
    "        output = self.clf_(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuxClf(\n",
       "  (feature_): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "    (1): BasicConv2d(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (clf_): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.7, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试辅助分类器————————————————————————————\n",
    "AuxClf(512,1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 建立GoogLeNet的架构\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/GoogleNet.PNG?versionId=CAEQFRiBgIDNw62XyhciIDA0ZDViOWM5YjYyNDRlZmJiMmMzNWVjOTZlNjk5YmMw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1的输入为image类型，通道为3\n",
    "# 计算conv1的padding (224+2p - 7)/2 +1 = 112.5 --> p = 3\n",
    "# maxpool1 (112-3)/2+1= 55.5 卷积和池化都是自动向下去成,可以设置ceil_model=True设置为向上取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————GoogLeNet的架构———————————————————————————\n",
    "class GoogLetNet(nn.Module):\n",
    "    def __init__(self,num_classes:int = 1000,blocks=None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, AuxClf]  # \n",
    "        conv_block = blocks[0]\n",
    "        Inception_block = blocks[1]\n",
    "        AuxClf_block = blocks[2]\n",
    "\n",
    "        # block1 \n",
    "        self.conv1 = conv_block(3,64,kernel_size=7,stride=2,padding=3)  # conv1的输入为image类型，通道为3\n",
    "        # 计算conv1的padding (224+2p - 7)/2 +1 = 112.5 --> p = 3     \n",
    "        self.maxpool1 = nn.MaxPool2d(3,stride=2,ceil_mode=True)  #  重叠池化\n",
    "        # maxpool1 (112-3)/2+1= 55.5 卷积和池化都是自动向下去成,可以设置ceil_model=True设置为向上取整\n",
    "\n",
    "        # block2\n",
    "        self.conv2 = conv_block(64,64,kernel_size=1)  # 1x1的卷积层并没有改变特征的尺寸，只是进行卷积操作\n",
    "        self.conv3 = conv_block(64,192,kernel_size=3,padding= 1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3,stride=2,ceil_mode=True)\n",
    "\n",
    "        # block3有2个Inception块\n",
    "        self.in3a = Inception_block(192,64,96,128,16,32,32)\n",
    "        self.in3b = Inception_block(256,128,128,192,32,96,64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3,stride=2,ceil_mode=True)\n",
    "\n",
    "        # block 4 5个Inception块\n",
    "        self.in4a = Inception_block(480,192,96,208,16,48,64)\n",
    "        self.in4b = Inception_block(512,160,112,224,24,64,64)\n",
    "        self.in4c = Inception_block(512,128,128,256,24,64,64)\n",
    "        self.in4d = Inception_block(512,112,144,288,32,64,64)\n",
    "        self.in4e = Inception_block(528,256,160,320,32,128,128)\n",
    "        self.maxpool4 = nn.MaxPool2d(3,stride=2,ceil_mode=True)\n",
    "\n",
    "         # block5\n",
    "        self.in5a = Inception_block(832,256,160,320,32,128,128)\n",
    "        self.in5b = Inception_block(832,384,192,384,48,128,128)\n",
    "\n",
    "        #clf\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))   # 自适应得池化层:只需填需要的特征图尺寸是多少\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024,num_classes)\n",
    "\n",
    "        # 辅助分类器\n",
    "        self.aux1 =AuxClf_block(512,num_classes)  # 4a后面的辅助分类器 4a输出512\n",
    "        self.aux2 = AuxClf_block(528,num_classes)  # 4d后面的辅助分类器\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # block 1\n",
    "        x = self.maxpool1(self.conv1(x))\n",
    "\n",
    "        # block 2 \n",
    "        x = self.maxpool2(self.conv3(self.conv2(x)))\n",
    "\n",
    "        # block3\n",
    "        x = self.in3a(x)\n",
    "        x = self.maxpool3(self.in3b(x))\n",
    "\n",
    "        # block 4 \n",
    "        x = self.in4a(x)\n",
    "        aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.in4b(x)\n",
    "        x = self.in4c(x)\n",
    "        x = self.in4d(x)\n",
    "        aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.in4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "\n",
    "\n",
    "        # block 5\n",
    "        x = self.in5a(x)\n",
    "        x = self.in5b(x)\n",
    "\n",
    "        # clf\n",
    "        x = self.avgpool(x) # 全局平均池化以后特征尺寸为1X1 \n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x , aux1, aux2  # 返回主分类器和两个辅助分类器的记过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000])\n",
      "torch.Size([10, 1000])\n",
      "torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "#————————————————————————————测试GoogLeNet————————————————————————————\n",
    "data = torch.ones(10,3,224,244)\n",
    "\n",
    "# 实例化\n",
    "net = GoogLetNet()\n",
    "\n",
    "fc2, fc1 ,fc0 = net(data)\n",
    "\n",
    "for i in [fc2, fc1, fc0]:\n",
    "    print(i.shape)\n",
    "\n",
    "# 返回1000个线性层的结果\n",
    "# 接下来应该放入损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "GoogLeNet                                     [10, 1000]                --\n",
       "├─BasicConv2d: 1-1                            [10, 64, 112, 112]        --\n",
       "│    └─Sequential: 2-1                        [10, 64, 112, 112]        --\n",
       "│    │    └─Conv2d: 3-1                       [10, 64, 112, 112]        9,408\n",
       "│    │    └─BatchNorm2d: 3-2                  [10, 64, 112, 112]        128\n",
       "│    │    └─ReLU: 3-3                         [10, 64, 112, 112]        --\n",
       "├─MaxPool2d: 1-2                              [10, 64, 56, 56]          --\n",
       "├─BasicConv2d: 1-3                            [10, 64, 56, 56]          --\n",
       "│    └─Sequential: 2-2                        [10, 64, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-4                       [10, 64, 56, 56]          4,096\n",
       "│    │    └─BatchNorm2d: 3-5                  [10, 64, 56, 56]          128\n",
       "│    │    └─ReLU: 3-6                         [10, 64, 56, 56]          --\n",
       "├─BasicConv2d: 1-4                            [10, 192, 56, 56]         --\n",
       "│    └─Sequential: 2-3                        [10, 192, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-7                       [10, 192, 56, 56]         110,592\n",
       "│    │    └─BatchNorm2d: 3-8                  [10, 192, 56, 56]         384\n",
       "│    │    └─ReLU: 3-9                         [10, 192, 56, 56]         --\n",
       "├─MaxPool2d: 1-5                              [10, 192, 28, 28]         --\n",
       "├─Inception: 1-6                              [10, 256, 28, 28]         --\n",
       "│    └─BasicConv2d: 2-4                       [10, 64, 28, 28]          --\n",
       "│    │    └─Sequential: 3-10                  [10, 64, 28, 28]          12,416\n",
       "│    └─Sequential: 2-5                        [10, 128, 28, 28]         --\n",
       "│    │    └─BasicConv2d: 3-11                 [10, 96, 28, 28]          18,624\n",
       "│    │    └─BasicConv2d: 3-12                 [10, 128, 28, 28]         110,848\n",
       "│    └─Sequential: 2-6                        [10, 32, 28, 28]          --\n",
       "│    │    └─BasicConv2d: 3-13                 [10, 16, 28, 28]          3,104\n",
       "│    │    └─BasicConv2d: 3-14                 [10, 32, 28, 28]          12,864\n",
       "│    └─Sequential: 2-7                        [10, 32, 28, 28]          --\n",
       "│    │    └─MaxPool2d: 3-15                   [10, 192, 28, 28]         --\n",
       "│    │    └─BasicConv2d: 3-16                 [10, 32, 28, 28]          6,208\n",
       "├─Inception: 1-7                              [10, 480, 28, 28]         --\n",
       "│    └─BasicConv2d: 2-8                       [10, 128, 28, 28]         --\n",
       "│    │    └─Sequential: 3-17                  [10, 128, 28, 28]         33,024\n",
       "│    └─Sequential: 2-9                        [10, 192, 28, 28]         --\n",
       "│    │    └─BasicConv2d: 3-18                 [10, 128, 28, 28]         33,024\n",
       "│    │    └─BasicConv2d: 3-19                 [10, 192, 28, 28]         221,568\n",
       "│    └─Sequential: 2-10                       [10, 96, 28, 28]          --\n",
       "│    │    └─BasicConv2d: 3-20                 [10, 32, 28, 28]          8,256\n",
       "│    │    └─BasicConv2d: 3-21                 [10, 96, 28, 28]          76,992\n",
       "│    └─Sequential: 2-11                       [10, 64, 28, 28]          --\n",
       "│    │    └─MaxPool2d: 3-22                   [10, 256, 28, 28]         --\n",
       "│    │    └─BasicConv2d: 3-23                 [10, 64, 28, 28]          16,512\n",
       "├─MaxPool2d: 1-8                              [10, 480, 14, 14]         --\n",
       "├─Inception: 1-9                              [10, 512, 14, 14]         --\n",
       "│    └─BasicConv2d: 2-12                      [10, 192, 14, 14]         --\n",
       "│    │    └─Sequential: 3-24                  [10, 192, 14, 14]         92,544\n",
       "│    └─Sequential: 2-13                       [10, 208, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-25                 [10, 96, 14, 14]          46,272\n",
       "│    │    └─BasicConv2d: 3-26                 [10, 208, 14, 14]         180,128\n",
       "│    └─Sequential: 2-14                       [10, 48, 14, 14]          --\n",
       "│    │    └─BasicConv2d: 3-27                 [10, 16, 14, 14]          7,712\n",
       "│    │    └─BasicConv2d: 3-28                 [10, 48, 14, 14]          19,296\n",
       "│    └─Sequential: 2-15                       [10, 64, 14, 14]          --\n",
       "│    │    └─MaxPool2d: 3-29                   [10, 480, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-30                 [10, 64, 14, 14]          30,848\n",
       "├─AuxClf: 1-10                                [10, 1000]                --\n",
       "│    └─Sequential: 2-16                       [10, 128, 4, 4]           --\n",
       "│    │    └─AvgPool2d: 3-31                   [10, 512, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-32                      [10, 128, 4, 4]           65,664\n",
       "│    └─Sequential: 2-17                       [10, 1000]                --\n",
       "│    │    └─Linear: 3-33                      [10, 1024]                2,098,176\n",
       "│    │    └─ReLU: 3-34                        [10, 1024]                --\n",
       "│    │    └─Dropout: 3-35                     [10, 1024]                --\n",
       "│    │    └─Linear: 3-36                      [10, 1000]                1,025,000\n",
       "├─Inception: 1-11                             [10, 512, 14, 14]         --\n",
       "│    └─BasicConv2d: 2-18                      [10, 160, 14, 14]         --\n",
       "│    │    └─Sequential: 3-37                  [10, 160, 14, 14]         82,240\n",
       "│    └─Sequential: 2-19                       [10, 224, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-38                 [10, 112, 14, 14]         57,568\n",
       "│    │    └─BasicConv2d: 3-39                 [10, 224, 14, 14]         226,240\n",
       "│    └─Sequential: 2-20                       [10, 64, 14, 14]          --\n",
       "│    │    └─BasicConv2d: 3-40                 [10, 24, 14, 14]          12,336\n",
       "│    │    └─BasicConv2d: 3-41                 [10, 64, 14, 14]          38,528\n",
       "│    └─Sequential: 2-21                       [10, 64, 14, 14]          --\n",
       "│    │    └─MaxPool2d: 3-42                   [10, 512, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-43                 [10, 64, 14, 14]          32,896\n",
       "├─Inception: 1-12                             [10, 512, 14, 14]         --\n",
       "│    └─BasicConv2d: 2-22                      [10, 128, 14, 14]         --\n",
       "│    │    └─Sequential: 3-44                  [10, 128, 14, 14]         65,792\n",
       "│    └─Sequential: 2-23                       [10, 256, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-45                 [10, 128, 14, 14]         65,792\n",
       "│    │    └─BasicConv2d: 3-46                 [10, 256, 14, 14]         295,424\n",
       "│    └─Sequential: 2-24                       [10, 64, 14, 14]          --\n",
       "│    │    └─BasicConv2d: 3-47                 [10, 24, 14, 14]          12,336\n",
       "│    │    └─BasicConv2d: 3-48                 [10, 64, 14, 14]          38,528\n",
       "│    └─Sequential: 2-25                       [10, 64, 14, 14]          --\n",
       "│    │    └─MaxPool2d: 3-49                   [10, 512, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-50                 [10, 64, 14, 14]          32,896\n",
       "├─Inception: 1-13                             [10, 528, 14, 14]         --\n",
       "│    └─BasicConv2d: 2-26                      [10, 112, 14, 14]         --\n",
       "│    │    └─Sequential: 3-51                  [10, 112, 14, 14]         57,568\n",
       "│    └─Sequential: 2-27                       [10, 288, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-52                 [10, 144, 14, 14]         74,016\n",
       "│    │    └─BasicConv2d: 3-53                 [10, 288, 14, 14]         373,824\n",
       "│    └─Sequential: 2-28                       [10, 64, 14, 14]          --\n",
       "│    │    └─BasicConv2d: 3-54                 [10, 32, 14, 14]          16,448\n",
       "│    │    └─BasicConv2d: 3-55                 [10, 64, 14, 14]          51,328\n",
       "│    └─Sequential: 2-29                       [10, 64, 14, 14]          --\n",
       "│    │    └─MaxPool2d: 3-56                   [10, 512, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-57                 [10, 64, 14, 14]          32,896\n",
       "├─AuxClf: 1-14                                [10, 1000]                --\n",
       "│    └─Sequential: 2-30                       [10, 128, 4, 4]           --\n",
       "│    │    └─AvgPool2d: 3-58                   [10, 528, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-59                      [10, 128, 4, 4]           67,712\n",
       "│    └─Sequential: 2-31                       [10, 1000]                --\n",
       "│    │    └─Linear: 3-60                      [10, 1024]                2,098,176\n",
       "│    │    └─ReLU: 3-61                        [10, 1024]                --\n",
       "│    │    └─Dropout: 3-62                     [10, 1024]                --\n",
       "│    │    └─Linear: 3-63                      [10, 1000]                1,025,000\n",
       "├─Inception: 1-15                             [10, 832, 14, 14]         --\n",
       "│    └─BasicConv2d: 2-32                      [10, 256, 14, 14]         --\n",
       "│    │    └─Sequential: 3-64                  [10, 256, 14, 14]         135,680\n",
       "│    └─Sequential: 2-33                       [10, 320, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-65                 [10, 150, 14, 14]         79,500\n",
       "│    │    └─BasicConv2d: 3-66                 [10, 320, 14, 14]         432,640\n",
       "│    └─Sequential: 2-34                       [10, 128, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-67                 [10, 32, 14, 14]          16,960\n",
       "│    │    └─BasicConv2d: 3-68                 [10, 128, 14, 14]         102,656\n",
       "│    └─Sequential: 2-35                       [10, 128, 14, 14]         --\n",
       "│    │    └─MaxPool2d: 3-69                   [10, 528, 14, 14]         --\n",
       "│    │    └─BasicConv2d: 3-70                 [10, 128, 14, 14]         67,840\n",
       "├─MaxPool2d: 1-16                             [10, 832, 7, 7]           --\n",
       "├─Inception: 1-17                             [10, 832, 7, 7]           --\n",
       "│    └─BasicConv2d: 2-36                      [10, 256, 7, 7]           --\n",
       "│    │    └─Sequential: 3-71                  [10, 256, 7, 7]           213,504\n",
       "│    └─Sequential: 2-37                       [10, 320, 7, 7]           --\n",
       "│    │    └─BasicConv2d: 3-72                 [10, 160, 7, 7]           133,440\n",
       "│    │    └─BasicConv2d: 3-73                 [10, 320, 7, 7]           461,440\n",
       "│    └─Sequential: 2-38                       [10, 128, 7, 7]           --\n",
       "│    │    └─BasicConv2d: 3-74                 [10, 32, 7, 7]            26,688\n",
       "│    │    └─BasicConv2d: 3-75                 [10, 128, 7, 7]           102,656\n",
       "│    └─Sequential: 2-39                       [10, 128, 7, 7]           --\n",
       "│    │    └─MaxPool2d: 3-76                   [10, 832, 7, 7]           --\n",
       "│    │    └─BasicConv2d: 3-77                 [10, 128, 7, 7]           106,752\n",
       "├─Inception: 1-18                             [10, 1024, 7, 7]          --\n",
       "│    └─BasicConv2d: 2-40                      [10, 384, 7, 7]           --\n",
       "│    │    └─Sequential: 3-78                  [10, 384, 7, 7]           320,256\n",
       "│    └─Sequential: 2-41                       [10, 384, 7, 7]           --\n",
       "│    │    └─BasicConv2d: 3-79                 [10, 192, 7, 7]           160,128\n",
       "│    │    └─BasicConv2d: 3-80                 [10, 384, 7, 7]           664,320\n",
       "│    └─Sequential: 2-42                       [10, 128, 7, 7]           --\n",
       "│    │    └─BasicConv2d: 3-81                 [10, 48, 7, 7]            40,032\n",
       "│    │    └─BasicConv2d: 3-82                 [10, 128, 7, 7]           153,856\n",
       "│    └─Sequential: 2-43                       [10, 128, 7, 7]           --\n",
       "│    │    └─MaxPool2d: 3-83                   [10, 832, 7, 7]           --\n",
       "│    │    └─BasicConv2d: 3-84                 [10, 128, 7, 7]           106,752\n",
       "├─AdaptiveAvgPool2d: 1-19                     [10, 1024, 1, 1]          --\n",
       "├─Dropout: 1-20                               [10, 1024]                --\n",
       "├─Linear: 1-21                                [10, 1000]                1,025,000\n",
       "===============================================================================================\n",
       "Total params: 13,351,460\n",
       "Trainable params: 13,351,460\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 15.84\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 516.60\n",
       "Params size (MB): 53.41\n",
       "Estimated Total Size (MB): 576.03\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net,(10,3,224,224),device='cpu')\n",
    "\n",
    "# 这个层次结构中至少有三层，最内部的是普通卷积层构成的分枝branch，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "GoogLeNet                                     [10, 1000]                --\n",
       "├─BasicConv2d: 1-1                            [10, 64, 112, 112]        9,536\n",
       "├─MaxPool2d: 1-2                              [10, 64, 56, 56]          --\n",
       "├─BasicConv2d: 1-3                            [10, 64, 56, 56]          4,224\n",
       "├─BasicConv2d: 1-4                            [10, 192, 56, 56]         110,976\n",
       "├─MaxPool2d: 1-5                              [10, 192, 28, 28]         --\n",
       "├─Inception: 1-6                              [10, 256, 28, 28]         164,064\n",
       "├─Inception: 1-7                              [10, 480, 28, 28]         389,376\n",
       "├─MaxPool2d: 1-8                              [10, 480, 14, 14]         --\n",
       "├─Inception: 1-9                              [10, 512, 14, 14]         376,800\n",
       "├─AuxClf: 1-10                                [10, 1000]                3,188,840\n",
       "├─Inception: 1-11                             [10, 512, 14, 14]         449,808\n",
       "├─Inception: 1-12                             [10, 512, 14, 14]         510,768\n",
       "├─Inception: 1-13                             [10, 528, 14, 14]         606,080\n",
       "├─AuxClf: 1-14                                [10, 1000]                3,190,888\n",
       "├─Inception: 1-15                             [10, 832, 14, 14]         835,276\n",
       "├─MaxPool2d: 1-16                             [10, 832, 7, 7]           --\n",
       "├─Inception: 1-17                             [10, 832, 7, 7]           1,044,480\n",
       "├─Inception: 1-18                             [10, 1024, 7, 7]          1,445,344\n",
       "├─AdaptiveAvgPool2d: 1-19                     [10, 1024, 1, 1]          --\n",
       "├─Dropout: 1-20                               [10, 1024]                --\n",
       "├─Linear: 1-21                                [10, 1000]                1,025,000\n",
       "===============================================================================================\n",
       "Total params: 13,351,460\n",
       "Trainable params: 13,351,460\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 15.84\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 516.60\n",
       "Params size (MB): 53.41\n",
       "Estimated Total Size (MB): 576.03\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————使用参数depth————————————————————————————\n",
    "\n",
    "summary(net,(10,3,224,224),device='cpu',depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.ResNet\n",
    "\n",
    "即便算法已经基本达到人类水平，但有两个瓶颈一直没有被突破：  \n",
    "1. 网络能够达到的最大深度依然很浅，VGG是19层，GoogLeNet也没有超过25层  \n",
    "2. 深度网络的训练难度太大，虽然强行堆叠卷积层或inception让网络加深非常容易，但加深后的网络往往收敛困难，损失很高，精度很低。\n",
    "\n",
    "\n",
    "- **训练很深的神经网络比创造很深的神经网络架构难得多**  \n",
    "  - 网络深度加深，精度却在降低，这种现象在深度网络的训练中被称为“退化”（degradation），退化现象的存在导致深度网络在实际任务上的表现常常会不如浅层网络。\n",
    "  - 深层网络中的函数关系本质上就比浅层网络中的函数关系更复杂、更难拟合，因此深层网络本质上就比浅层网络更难优化和训练。\n",
    "\n",
    "\n",
    ">在现有优化算法、优化思路下，在浅层网络后增加恒等函数很可能就是最优的（optimal）加深网络深度的方式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残差网络ResNet基本思想：假设增加深度用的最优结构就是恒等函数，利用恒等函数的性质，将用于加深网络深度的结构向更容易拟合和训练的方向设计。\n",
    "\n",
    "具体怎么操作呢？VGG用来加深网络的结构是重复的卷积层，GoogLeNet用来加深网络的结构是Inception块。  \n",
    "而在残差网络中，这个结构块是“残差块”（Residual unit），也可以译作残差单元。在残差网络中,我们将众多残差单元与普通卷积层串联，以实现“在浅层网络后堆叠某种结构、以增加深度”的目的。\n",
    "\n",
    "\n",
    "无论残差单元拥有怎样的结构，它一定也存在输入值$x$和必须拟合的关系$H(x)$。假设我们使用$F(x)$来表示输入值$x$与输出的函数关系$H(x)$之间的差异，则有$F(x)=H(x)-x$，此时F(x)就是残差（Residual）。\n",
    "- 拟合效果较好时，$F(x)$取值为0，拟合0与$F(x)$的关系，比拟合一个未知的函数$F(x)$与$H(x)$的关系要容易\n",
    "\n",
    "![](https://gitee.com/bravojimoon/note-picture/raw/master/torch/%7DQX%7BX09NSKQ2%5DU%5DOD3_2%5DFV.png)\n",
    "\n",
    "\n",
    "\n",
    "- 残差网络的优势\n",
    "  - 残差单元几乎实现了0负担增加深度。首先，跳跃连接不带有任何参数，普通卷积层的结构也不复杂，因此残差块的增加不会给模型带来太多额外的参数负担。同时，由于残差单元比普通网络更容易训练，并且在理论上能够保持网路的精度，因此残差网络的深度可以大幅增加\n",
    "  - 残差单元还可以大幅加速训练和运算速度\n",
    "    - 残差单元可以在上层卷积层还未训练的时候就迅速将数据信息传递到下一层中。\n",
    "    - 因为卷积层接近于恒等函数，在对残差网络进行反向传播时，梯度也可以更快速地通过跳跃链接从后往前传递。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们就来复现一下残差网络，我们将创造一个通用的类，在这个类上输入相关的参数，就可以实现上图中展现的五种残差网络。\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/ResNet架构.PNG?versionId=CAEQFRiBgIDQw62XyhciIDUzZmRhZGJhNThjNTQ3NzhhZjAxOGFjYWM1MjYxZmMw)\n",
    "\n",
    "\n",
    "和复现GoogLeNet时一样，我们先从简单的、可以打包的元素开始定义。残差网络中的卷积层虽然变化多端，但其实只有两种：3x3卷积层与1x1卷积层，并且我们知道，每个卷积层后面都需要跟上BN层，而BN层上可以完成参数初始化。我们就从这里开始写：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  basicconv - conv2d + BN + ReLU (-> conv3x3, conv1x1 )\n",
    "# Residual Unit, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Type,Union,List,Optional\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/35.PNG?versionId=CAEQFRiBgID07vWryhciIDRhYTkwODA1MDU0YjRmNWQ5ZTk2NzAxMjdkYzE2MGZm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————定义3x3的卷积————————————————————————————\n",
    "# 大部分的3x3卷积都不改变数据结构\n",
    "def conv3x3(in_,out_,stride=1,initialzero=False):\n",
    "\n",
    "    bn = nn.BatchNorm2d(out_)\n",
    "    #需要进行判断：要对BN进行0初始化吗？\n",
    "    #最后一层就初始化,不是最后一层就不改变gamma和beta\n",
    "    \n",
    "    if initialzero == True:\n",
    "        nn.init.constant_(bn.weight,0)  # 修改bn层的weight\n",
    "    return nn.Sequential(nn.Conv2d(in_,out_,kernel_size=3,padding=1,stride=stride,bias=False),\n",
    "                 bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "conv3x3(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————定义1x1的卷积————————————————————————————\n",
    "# 和3x3类似，只是kernel_size和padding不同\n",
    "\n",
    "def conv1x1(in_,out_,stride=1,initialzero=False):\n",
    "    bn = nn.BatchNorm2d(out_)\n",
    "    #需要进行判断：要对BN进行0初始化吗？\n",
    "    #最后一层就初始化,不是最后一层就不改变gamma和beta\n",
    "    if initialzero == True:\n",
    "        nn.init.constant_(bn.weight,0)  # 修改bn层的weight\n",
    "    return nn.Sequential(nn.Conv2d(in_,out_,kernel_size=1,padding=0,stride=stride,bias=False),\n",
    "                 bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 执行0初始化\n",
    "conv1x1(2,10,1,True)[1].weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 残差单元类\n",
    "  - 两个部分的并联 -> 卷积和跳跃连接\n",
    "  - 一个残差单元中只包含两个卷积层和一个加和功能。\n",
    "\n",
    "1. 初始化  \n",
    "&emsp;&emsp;初始化只会发生在每个残差单元最后一个卷积层的bn层上，因此我们将最后一个层的参数initialzero设置为\n",
    "True，其他地方不做修改，因此其他卷积层中的0初始化功能并未开启。\n",
    "2. 步长  \n",
    "&emsp;&emsp;残差网络使用stride=2的卷积层来给特征图降维，并且每次降维都发生在layers与layers之间。\n",
    "3. 特征图数量的变化  \n",
    "&emsp;&emsp;在使用残差单元的浅层残差网络里，每当我们利用步长=2来缩小特征图尺寸，特征图的数量也会翻倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————————————————定义：残差单元类————————————————————————————\n",
    "# \n",
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self,out_:int,stride1 :int =1,\n",
    "                in_: Optional[int] = None   # 添加参数in_ 并不影响后面代码\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stride1 = stride1\n",
    "        # 当特征图尺寸需要缩小时，卷积层的输出特征图数量out等于输入特征图in的2倍\n",
    "        # 不需要缩小时, out = in\n",
    "        if stride1 != 1:\n",
    "            in_ = int(out_ / 2)\n",
    "        else:\n",
    "            in_ = out_\n",
    "\n",
    "\n",
    "        # stride1是否等于2呢？如果等于 2-特征图尺寸会发生变化\n",
    "        # 需要在跳跃连接上增加1x1卷积层来调整特征图尺寸\n",
    "        # 如果stride1等于1,则说明都不需要做\n",
    "\n",
    "        # 拟合部分，输入F(x)\n",
    "        self.fit_ = nn.Sequential(conv3x3(in_,out_,stride=stride1),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  conv3x3(out_,out_,initialzero=True)  # 最后一个卷积层一定使用bn层初始化\n",
    "                                  )\n",
    "\n",
    "        # 跳跃连接，输出1x1卷积核之后的x\n",
    "        self.skipconv = conv1x1(in_,out_,stride=stride1)\n",
    "\n",
    "        # 单独定义放在H(x)之后来使用的激活函数ReLU\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx = self.fit_(x)   # 拟合结果\n",
    "        if self.stride1 != 1:    # 跳跃连接\n",
    "            x = self.skipconv(x) \n",
    "\n",
    "        hx = self.relu(fx+x)\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 28, 28])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 56, 56])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "TypeError",
     "evalue": "ResidualUnit.__init__() got an unexpected keyword argument 'in_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2216\\2755532653.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#你是否注意到in_的存在？\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mru\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResidualUnit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0min_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#输入in_不影响ResidualUnit的任何行为\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ResidualUnit.__init__() got an unexpected keyword argument 'in_'"
     ]
    }
   ],
   "source": [
    "#————————————————————————————测试：残差单元————————————————————————————\n",
    "\n",
    "data = torch.ones(10,64,56,56) #特征图尺寸64x64，输入特征图数量64\n",
    "conv3_x_18_0 = ResidualUnit(128,stride1=2) #特征图尺寸折半，特征图数量加倍\n",
    "conv3_x_18_0(data).shape\n",
    "\n",
    "conv2_x_18_0 = ResidualUnit(64) #特征图尺寸不变，特征图数量也不变\n",
    "conv2_x_18_0(data).shape\n",
    "\n",
    "#你是否注意到in_的存在？\n",
    "ru = ResidualUnit(64,in_ = 64) #输入in_不影响ResidualUnit的任何行为\n",
    "ru(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 瓶颈结构\n",
    "\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/ResNet架构.PNG?versionId=CAEQFRiBgIDQw62XyhciIDUzZmRhZGJhNThjNTQ3NzhhZjAxOGFjYWM1MjYxZmMw)\n",
    "\n",
    "- in_:Optional[int] 选填参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,middle_out,\n",
    "                stride1 = 1,\n",
    "                in_:Optional[int]=None) -> None:\n",
    "        \"\"\"\n",
    "        in_：输入瓶颈结构的特征图数量，仅在conv1之后紧跟的瓶颈结构才进行填写。其他时候不填写。\n",
    "       stride1：第一个卷积层/跳跃连接中1x1卷积层的步长\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.stride1 = stride1\n",
    "\n",
    "        # 利用一个参数middle_out 表达了2个参数 in和out\n",
    "        # 最终的数据量 = 中间输出量4倍\n",
    "        out_ = 4 *middle_out\n",
    "\n",
    "        # 我希望使用选填参数in_来帮助我们区别这个架构是不是在conv1之后\n",
    "        # 如果不是紧跟在conv1后，就不填写in_ ; 如果是 就填写in_ = 64 \n",
    "        if in_ == None:\n",
    "        # 是否需要将特征图的尺寸缩小的场合吗？\n",
    "        # conv2_x - conv_3x - conv_4x - conv_5x 相互链接的时候\n",
    "        # 每次都需要将特征图的尺寸折半，同时卷积层上的middle_out = 1/2 in_\n",
    "        #需要缩小特征图，则输入量 = 中间输出量 * 2  \n",
    "            #不需要缩小特征图，则输入量 = 中间输出量 * 4\n",
    "            if stride1 != 1:  # 缩小特征图的场合，这个瓶颈结构是每个layers的第一个瓶颈结构\n",
    "                in_ = middle_out *2\n",
    "            else:             # 不缩小特征图的场合，是在第一个瓶颈结构之后的重复结构\n",
    "                in_ = middle_out * 4\n",
    "\n",
    "\n",
    "        \n",
    "        self.fit_ = nn.Sequential(conv1x1(in_,middle_out,stride=stride1),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  conv3x3(middle_out,middle_out),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  conv1x1(middle_out,out_,initialzero=True))\n",
    "\n",
    "        self.skipconv = conv1x1(in_,out_,stride=stride1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        fx = self.fit_(x)\n",
    "        # 跳跃连接\n",
    "        x = self.skipconv(x)\n",
    "        hx = self.relu(x+fx)\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 56, 56])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试：瓶颈结构 - conv1后紧跟的第一个瓶颈结构————————————————————————————\n",
    "\n",
    "data1 = torch.ones(10,64,56,56) #特征图尺寸56x56，特征图数量64\n",
    "#是conv1后紧跟的第一个瓶颈结构\n",
    "conv2_x_101_0 = Bottleneck(in_ = 64, middle_out = 64) \n",
    "conv2_x_101_0(data1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 28, 28])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#—————————测试：瓶颈结构 -不是conv1后紧跟的第一个瓶颈结构，但是需要缩小特征图尺寸——————————————\n",
    "\n",
    "data2 = torch.ones(10,256,56,56)\n",
    "#不是conv1后紧跟的第一个瓶颈结构，但是需要缩小特征图尺寸\n",
    "conv3_x_101_0 = Bottleneck(middle_out = 128, stride1=2)\n",
    "conv3_x_101_0(data2).shape #输出翻2倍并缩小特征图尺寸至一半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 28, 28])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#—————————测试：瓶颈结构 -不是conv1后紧跟的第一个瓶颈结构，也不需要缩小特征图尺寸——————————————\n",
    "\n",
    "\n",
    "data3 = torch.ones(10,512,28,28)\n",
    "#不是conv1后紧跟的第一个瓶颈结构，也不需要缩小特征图尺寸\n",
    "conv3_x_101_1 = Bottleneck(128)\n",
    "conv3_x_101_1(data3).shape #输出不变，特征图尺寸也不变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难发现，虽然每个类内部的逻辑需要进行一些梳理，但我们完成的类只有3个参数：这个块中的输出的特征图数量/中间输出量，这个块中第一个卷积层的步长，以及选填的输入特征图数量。\n",
    "\n",
    "对于同一个layer，残差块中的输出特征图数目 = 瓶颈架构中的中间输出量，而步长其实隐性地决定了这个block在架构中是否位于需要降低特征图尺寸的位置。现在，我们需要将这两个类打包到一个更高级的类中，用来生成每个layers中所有的blocks。\n",
    "\n",
    "*class Bottleneck(middle_out, stride1, in_(optional))*  \n",
    "*class ResidualUnit(out_, stride1, in_(optional))*\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/ResNet架构.PNG?versionId=CAEQFRiBgIDQw62XyhciIDUzZmRhZGJhNThjNTQ3NzhhZjAxOGFjYWM1MjYxZmMw)\n",
    "\n",
    "\n",
    "观察架构图。残差网络的每个layers中都存在大量重复的元素，在深层残差网络中，conv4_x中甚至将瓶颈架构重复了30次以上，如果能够使用代表数量的参数和for循环来对重复的部分进行控制，我们就可以大幅提高生成网络的效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualUnit(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "ResidualUnit(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "ResidualUnit(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 34 layer - conv3_x\n",
    "ru0 = ResidualUnit(out_=1215,stride1=2)  \n",
    "ru1 = ResidualUnit(out_=128)\n",
    "ru2 = ResidualUnit(out_=128)\n",
    "ru4 = ResidualUnit(out_=128)\n",
    "\n",
    "#————————————————————————————上述代码可以用循环实现————————————————————————————\n",
    "ru0_ = ResidualUnit(out_=1215,stride1=2)  \n",
    "\n",
    "num_blocks_conv3x = 4\n",
    "\n",
    "for i in range(num_blocks_conv3x - 1):\n",
    "    print(ResidualUnit(out_=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bottleneck(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "), Bottleneck(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "), Bottleneck(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "), Bottleneck(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "), Bottleneck(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "), Bottleneck(\n",
      "  (fit_): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (skipconv): Sequential(\n",
      "    (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "# 50 layer - conv4_x\n",
    "bt0 = Bottleneck(middle_out=256,stride1=2)\n",
    "bt1 = Bottleneck(middle_out=256)\n",
    "bt2 = Bottleneck(middle_out=256)\n",
    "# ...\n",
    "bt5 = num_blocks_conv3x - 1\n",
    "\n",
    "#————————————————————————————上述代码也可以用循环实现————————————————————————————\n",
    "\n",
    "num_blocks_conv4x = 6\n",
    "bt0 = Bottleneck(middle_out=256,stride1=2)\n",
    "# 在列表中添加第个瓶颈架构快\n",
    "conv4_x50 = []\n",
    "conv4_x50.append(Bottleneck(middle_out=256,stride1=1))\n",
    "\n",
    "for i in range(num_blocks_conv4x - 1):\n",
    "    conv4_x50.append(Bottleneck(middle_out=256))\n",
    "\n",
    "print(conv4_x50)\n",
    "\n",
    "\n",
    "\n",
    "# 考虑打包为一个函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第一个块\n",
    "  - if 这个块是conv1之后的参数:\n",
    "    - 需要一个特殊的参数 in_ = 64\n",
    "  - else: \n",
    "    - 第一层需要的独特的参数 stride1 = 2 \n",
    "- 剩下的块就使用for 循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试：用于瓶颈结构————————————————————————————\n",
    "from numpy import append\n",
    "\n",
    "\n",
    "layers = []\n",
    "num_blocks = 6\n",
    "afterconv1 = True  # 是否为conv1后的第一个快\n",
    "\n",
    "if afterconv1 == True:\n",
    "    layers.append(Bottleneck(middle_out=64,in_=64))\n",
    "else:\n",
    "    layers.append(Bottleneck(middle_out=128,stride1=2))\n",
    "\n",
    "for i in range(num_blocks - 1):\n",
    "    layers.append(Bottleneck(middle_out=128))\n",
    "\n",
    "len(layers)\n",
    "# layers 包含了6个类\n",
    "\n",
    "# 不能用于残差单元...因为残差单元中没有参数in_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————用于残差单元————————————————————————————\n",
    "# 在残差单元定义类时，添加参数in_,并且也将其设置为optional\n",
    "layers = []\n",
    "num_blocks = 6\n",
    "afterconv1 = True  # 是否为conv1后的第一个块\n",
    "\n",
    "\n",
    "if afterconv1 == True:\n",
    "    layers.append(ResidualUnit(out_=64,in_=64))\n",
    "else:\n",
    "    layers.append(ResidualUnit(out_=64,stride1=2))\n",
    "\n",
    "for i in range(num_blocks - 1):\n",
    "    layers.append(ResidualUnit(out_=128))\n",
    "\n",
    "len(layers)\n",
    "# layers 包含了6个类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Type[Union[ResidualUnit,Bottleneck]\n",
    "  - Type[ ]  表示只能填写的就是类\n",
    "  - Union[ ] 输入多个类，需要用Union连起来\n",
    "\n",
    "- SyntaxError: non-default argument follows default argument\n",
    "  - 非默认值的参数跟在了默认值参数的后面 错误\n",
    "  - 先写没有默认值的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个layers内的全部块打包到一个列表中\n",
    "# 专门用于生成ResNet中的每一个layers中的函数\n",
    "\n",
    "def make_layers(block: Type[Union[ResidualUnit,Bottleneck]], # 只能填入类，且只能填入RU BN之一\n",
    "                middle_out:int,\n",
    "                num_blocks :int,\n",
    "                afterconv1: bool=False):\n",
    "\n",
    "    '''\n",
    "   构建残差网络中layers的类\n",
    "    \n",
    "   block: 架构块的类型，可选ResidualUnit或Bottleneck。依据选择的架构块类型，可判断该残差网络的深浅\n",
    "   middle_out: ResidualUnit中输出的特征图数目/Bottleneck中的中间输出量，对两个block可混用\n",
    "   blocks：这个layer中的block数量\n",
    "   afterconv1：这个layer是否紧接在conv1之后？\n",
    "   '''\n",
    "    layers = []\n",
    "    \n",
    "    if afterconv1 == True:    # 是否为conv1之后的第一个block\n",
    "        layers.append(block(middle_out,in_=64))\n",
    "    else:\n",
    "        layers.append(block(middle_out,stride1=2))\n",
    "\n",
    "    for i in range(num_blocks - 1):   # 重复残差单元 / 瓶颈架构\n",
    "        layers.append(block(middle_out))\n",
    "    \n",
    "    return nn.Sequential(* layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试函数：make_layers————————————————————————————\n",
    "layer_34_conv4_x = make_layers(ResidualUnit,256,6,False)\n",
    "len(layer_34_conv4_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 星号解析列表/储存器 `nn.Sequential(* layer_34_conv4_x) `\n",
    "  - layer_34_conv4_x 是一个列表不能放到 nn.Sequential中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (2): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (3): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (4): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (5): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential(layer_34_conv4_x)  TypeError: list is not a Module subclass\n",
    "\n",
    "nn.Sequential(* layer_34_conv4_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (2): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [10, 64, 56, 56]          --\n",
       "├─ResidualUnit: 1-1                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-2                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-3                      [10, 64, 56, 56]          78,208\n",
       "==========================================================================================\n",
       "Total params: 234,624\n",
       "Trainable params: 234,624\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.94\n",
       "==========================================================================================\n",
       "Input size (MB): 8.03\n",
       "Forward/backward pass size (MB): 192.68\n",
       "Params size (MB): 0.89\n",
       "Estimated Total Size (MB): 201.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————4个测试————————————————————————————\n",
    "# 测试 - 需要分别对残差块 和 瓶颈架构进行测试，并且需要对conv1后的首个架构，以及中间的架构进行测试\n",
    "#注意检查：输入的数据结构是否正确，网络能否允许正确的数据结构输入，输入后产出的结构是否正确，\n",
    "# 包括特征图尺寸是否变化、特征图数量是否变化，以及一个layers中所包含的blocks数量是否正确\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (2): ResidualUnit(\n",
       "    (fit_): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (skipconv): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [10, 64, 56, 56]          --\n",
       "├─ResidualUnit: 1-1                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-2                      [10, 64, 56, 56]          78,208\n",
       "├─ResidualUnit: 1-3                      [10, 64, 56, 56]          78,208\n",
       "==========================================================================================\n",
       "Total params: 234,624\n",
       "Trainable params: 234,624\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.94\n",
       "==========================================================================================\n",
       "Input size (MB): 8.03\n",
       "Forward/backward pass size (MB): 192.68\n",
       "Params size (MB): 0.89\n",
       "Estimated Total Size (MB): 201.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试1:残差块后的首个架构————————————————————————————\n",
    "\n",
    "# 34层网络，conv2_x,紧跟在conv1后的首页个架构\n",
    "# 不改变特征图尺寸，且每层的输出都是64\n",
    "make_layers(ResidualUnit,128,3,afterconv1=True)\n",
    "\n",
    "conv2_x_34 = make_layers(ResidualUnit,64,3,afterconv1=True)\n",
    "\n",
    "datashape = (10,64,56,56)\n",
    "\n",
    "summary(conv2_x_34,datashape,depth=1,device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [10, 256, 7, 7]           --\n",
       "├─ResidualUnit: 1-1                      [10, 256, 7, 7]           919,040\n",
       "├─ResidualUnit: 1-2                      [10, 256, 7, 7]           1,246,720\n",
       "├─ResidualUnit: 1-3                      [10, 256, 7, 7]           1,246,720\n",
       "├─ResidualUnit: 1-4                      [10, 256, 7, 7]           1,246,720\n",
       "├─ResidualUnit: 1-5                      [10, 256, 7, 7]           1,246,720\n",
       "├─ResidualUnit: 1-6                      [10, 256, 7, 7]           1,246,720\n",
       "==========================================================================================\n",
       "Total params: 7,152,640\n",
       "Trainable params: 7,152,640\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.34\n",
       "==========================================================================================\n",
       "Input size (MB): 1.00\n",
       "Forward/backward pass size (MB): 26.09\n",
       "Params size (MB): 27.29\n",
       "Estimated Total Size (MB): 54.38\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试2————————————————————————————\n",
    "#34层网络，conv4_x，缩小特征图尺寸，且每层的输出翻倍\n",
    "datashape2 = (10,128,14,14)\n",
    "conv2_x_34 = make_layers(ResidualUnit,256, 6, afterconv1=False)\n",
    "summary(conv2_x_34,datashape2,depth=1,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [10, 256, 56, 56]         --\n",
       "├─Bottleneck: 1-1                        [10, 256, 56, 56]         --\n",
       "│    └─Sequential: 2-1                   [10, 256, 56, 56]         --\n",
       "│    │    └─Sequential: 3-1              [10, 64, 56, 56]          4,224\n",
       "│    │    └─ReLU: 3-2                    [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-3              [10, 64, 56, 56]          36,992\n",
       "│    │    └─ReLU: 3-4                    [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-5              [10, 256, 56, 56]         16,896\n",
       "│    └─Sequential: 2-2                   [10, 256, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-6                  [10, 256, 56, 56]         16,384\n",
       "│    │    └─BatchNorm2d: 3-7             [10, 256, 56, 56]         512\n",
       "│    └─ReLU: 2-3                         [10, 256, 56, 56]         --\n",
       "├─Bottleneck: 1-2                        [10, 256, 56, 56]         --\n",
       "│    └─Sequential: 2-4                   [10, 256, 56, 56]         --\n",
       "│    │    └─Sequential: 3-8              [10, 64, 56, 56]          16,512\n",
       "│    │    └─ReLU: 3-9                    [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-10             [10, 64, 56, 56]          36,992\n",
       "│    │    └─ReLU: 3-11                   [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-12             [10, 256, 56, 56]         16,896\n",
       "│    └─Sequential: 2-5                   [10, 256, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-13                 [10, 256, 56, 56]         65,536\n",
       "│    │    └─BatchNorm2d: 3-14            [10, 256, 56, 56]         512\n",
       "│    └─ReLU: 2-6                         [10, 256, 56, 56]         --\n",
       "├─Bottleneck: 1-3                        [10, 256, 56, 56]         --\n",
       "│    └─Sequential: 2-7                   [10, 256, 56, 56]         --\n",
       "│    │    └─Sequential: 3-15             [10, 64, 56, 56]          16,512\n",
       "│    │    └─ReLU: 3-16                   [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-17             [10, 64, 56, 56]          36,992\n",
       "│    │    └─ReLU: 3-18                   [10, 64, 56, 56]          --\n",
       "│    │    └─Sequential: 3-19             [10, 256, 56, 56]         16,896\n",
       "│    └─Sequential: 2-8                   [10, 256, 56, 56]         --\n",
       "│    │    └─Conv2d: 3-20                 [10, 256, 56, 56]         65,536\n",
       "│    │    └─BatchNorm2d: 3-21            [10, 256, 56, 56]         512\n",
       "│    └─ReLU: 2-9                         [10, 256, 56, 56]         --\n",
       "==========================================================================================\n",
       "Total params: 347,904\n",
       "Trainable params: 347,904\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.79\n",
       "==========================================================================================\n",
       "Input size (MB): 8.03\n",
       "Forward/backward pass size (MB): 963.38\n",
       "Params size (MB): 1.39\n",
       "Estimated Total Size (MB): 972.80\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试3:瓶颈架构————————————————————————————\n",
    "# 101层网络，conv2_x,紧跟在conv1后的首页个架构\n",
    "# 不改变特征图尺寸，且每层的输出都是64\n",
    "\n",
    "\n",
    "conv2_x_101 = make_layers(Bottleneck,64,3,afterconv1=True)\n",
    "\n",
    "datashape = (10,64,56,56)\n",
    "\n",
    "summary(conv2_x_101,datashape,depth=3,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [10, 1024, 14, 14]        --\n",
       "├─Bottleneck: 1-1                        [10, 1024, 14, 14]        1,512,448\n",
       "├─Bottleneck: 1-2                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-3                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-4                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-5                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-6                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-7                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-8                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-9                        [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-10                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-11                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-12                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-13                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-14                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-15                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-16                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-17                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-18                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-19                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-20                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-21                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-22                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Bottleneck: 1-23                       [10, 1024, 14, 14]        2,167,808\n",
       "==========================================================================================\n",
       "Total params: 49,204,224\n",
       "Trainable params: 49,204,224\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 96.21\n",
       "==========================================================================================\n",
       "Input size (MB): 16.06\n",
       "Forward/backward pass size (MB): 1846.48\n",
       "Params size (MB): 196.82\n",
       "Estimated Total Size (MB): 2059.35\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试4：瓶颈架构————————————————————————————\n",
    "#101层网络，conv4_x，缩小特征图尺寸，且每层的输出翻4倍\n",
    "datashape3 = (10,512,28,28)\n",
    "conv4_x_101 = make_layers(Bottleneck,256,23,afterconv1=False)\n",
    "summary(conv4_x_101,datashape3,depth=1,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经具备了构建layers的能力，可以开始构建自己的残差网络了。定义残差网络的类ResNet可能是所有复现步骤中最简单的一个，它与我们之前熟悉的其他网络的定义方式非常类似。参照架构图，定义残差网络的代码如下：\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2021PyTorchDL/WEEK9/ResNet架构.PNG?versionId=CAEQFRiBgIDQw62XyhciIDUzZmRhZGJhNThjNTQ3NzhhZjAxOGFjYWM1MjYxZmMw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立18层网络 layers = [2,2,2,2]\n",
    "# 建立32层网络： layes = [3,4,6,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block:Type[Union[ResidualUnit, Bottleneck]],\n",
    "                layers:List[int],\n",
    "                num_classes:int) -> None:\n",
    "\n",
    "        '''\n",
    "       block：要使用的用来加深深度的基本架构是？可以选择残差单元或瓶颈结构，两种都带有skip connection\n",
    "       layers：列表，每个层里具体有多少个块呢？可参考网络架构图。例如，34层的残差网络的layers = [3,4,6,3]\n",
    "       num_classes：真实标签含有多少个类别？\n",
    "       '''\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        # layer 1:卷积+池化\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64,7,stride=2,padding=3,bias=False),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPool2d(3,stride=2,ceil_mode=True))\n",
    "        # layer 2 - 5: 残差块 / 瓶颈架构\n",
    "        self.layer2_x = make_layers(block,64,layers[0],afterconv1=True) \n",
    "        self.layer3_x = make_layers(block,128,layers[1]) \n",
    "        self.layer4_x = make_layers(block,256,layers[2]) \n",
    "        self.layer5_x = make_layers(block,512,layers[3]) \n",
    "\n",
    "        # 全局平均池化\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # 分类 - 使用瓶颈架构的深度残差网络最终的输出值包含2048个像素,而浅层残差网络最终的输出值包含512个像素\n",
    "        if block == ResidualUnit:\n",
    "            self.fc = nn.Linear(512,num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(2048,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)   # layer1,普通卷积+池化输出\n",
    "        x = self.layer5_x(self.layer4_x(self.layer3_x(self.layer2_x(x))))\n",
    "        x = self.avgpool(x)  # 特征图的尺寸为1*1*sample\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNet                                        --                        --\n",
       "├─Sequential: 1-1                             [10, 64, 56, 56]          --\n",
       "│    └─Conv2d: 2-1                            [10, 64, 112, 112]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [10, 64, 112, 112]        128\n",
       "│    └─ReLU: 2-3                              [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [10, 64, 56, 56]          --\n",
       "├─Sequential: 1-2                             [10, 64, 56, 56]          --\n",
       "│    └─ResidualUnit: 2-5                      [10, 64, 56, 56]          78,208\n",
       "│    └─ResidualUnit: 2-6                      [10, 64, 56, 56]          78,208\n",
       "│    └─ResidualUnit: 2-7                      [10, 64, 56, 56]          78,208\n",
       "├─Sequential: 1-3                             [10, 128, 28, 28]         --\n",
       "│    └─ResidualUnit: 2-8                      [10, 128, 28, 28]         230,144\n",
       "│    └─ResidualUnit: 2-9                      [10, 128, 28, 28]         312,064\n",
       "│    └─ResidualUnit: 2-10                     [10, 128, 28, 28]         312,064\n",
       "│    └─ResidualUnit: 2-11                     [10, 128, 28, 28]         312,064\n",
       "├─Sequential: 1-4                             [10, 256, 14, 14]         --\n",
       "│    └─ResidualUnit: 2-12                     [10, 256, 14, 14]         919,040\n",
       "│    └─ResidualUnit: 2-13                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-14                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-15                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-16                     [10, 256, 14, 14]         1,246,720\n",
       "│    └─ResidualUnit: 2-17                     [10, 256, 14, 14]         1,246,720\n",
       "├─Sequential: 1-5                             [10, 512, 7, 7]           --\n",
       "│    └─ResidualUnit: 2-18                     [10, 512, 7, 7]           3,673,088\n",
       "│    └─ResidualUnit: 2-19                     [10, 512, 7, 7]           4,983,808\n",
       "│    └─ResidualUnit: 2-20                     [10, 512, 7, 7]           4,983,808\n",
       "├─AdaptiveAvgPool2d: 1-6                      [10, 512, 1, 1]           --\n",
       "├─Linear: 1-7                                 [10, 1000]                513,000\n",
       "===============================================================================================\n",
       "Total params: 22,716,840\n",
       "Trainable params: 22,716,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 36.64\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 598.18\n",
       "Params size (MB): 87.19\n",
       "Estimated Total Size (MB): 691.39\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试:建立34层的残差网络————————————————————————————\n",
    "datashape = (10,3,224,224)\n",
    "\n",
    "\n",
    "res34 = ResNet(ResidualUnit,[3,4,6,3],1000)\n",
    "summary(res34,datashape,depth=2,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNet                                        --                        --\n",
       "├─Sequential: 1-1                             [10, 64, 56, 56]          --\n",
       "│    └─Conv2d: 2-1                            [10, 64, 112, 112]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [10, 64, 112, 112]        128\n",
       "│    └─ReLU: 2-3                              [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [10, 64, 56, 56]          --\n",
       "├─Sequential: 1-2                             [10, 256, 56, 56]         --\n",
       "│    └─Bottleneck: 2-5                        [10, 256, 56, 56]         75,008\n",
       "│    └─Bottleneck: 2-6                        [10, 256, 56, 56]         136,448\n",
       "│    └─Bottleneck: 2-7                        [10, 256, 56, 56]         136,448\n",
       "├─Sequential: 1-3                             [10, 512, 28, 28]         --\n",
       "│    └─Bottleneck: 2-8                        [10, 512, 28, 28]         379,392\n",
       "│    └─Bottleneck: 2-9                        [10, 512, 28, 28]         543,232\n",
       "│    └─Bottleneck: 2-10                       [10, 512, 28, 28]         543,232\n",
       "│    └─Bottleneck: 2-11                       [10, 512, 28, 28]         543,232\n",
       "├─Sequential: 1-4                             [10, 1024, 14, 14]        --\n",
       "│    └─Bottleneck: 2-12                       [10, 1024, 14, 14]        1,512,448\n",
       "│    └─Bottleneck: 2-13                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-14                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-15                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-16                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-17                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-18                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-19                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-20                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-21                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-22                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-23                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-24                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-25                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-26                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-27                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-28                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-29                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-30                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-31                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-32                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-33                       [10, 1024, 14, 14]        2,167,808\n",
       "│    └─Bottleneck: 2-34                       [10, 1024, 14, 14]        2,167,808\n",
       "├─Sequential: 1-5                             [10, 2048, 7, 7]          --\n",
       "│    └─Bottleneck: 2-35                       [10, 2048, 7, 7]          6,039,552\n",
       "│    └─Bottleneck: 2-36                       [10, 2048, 7, 7]          8,660,992\n",
       "│    └─Bottleneck: 2-37                       [10, 2048, 7, 7]          8,660,992\n",
       "├─AdaptiveAvgPool2d: 1-6                      [10, 2048, 1, 1]          --\n",
       "├─Linear: 1-7                                 [10, 1000]                2,049,000\n",
       "===============================================================================================\n",
       "Total params: 76,981,288\n",
       "Trainable params: 76,981,288\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 135.30\n",
       "===============================================================================================\n",
       "Input size (MB): 6.02\n",
       "Forward/backward pass size (MB): 3701.06\n",
       "Params size (MB): 307.93\n",
       "Estimated Total Size (MB): 4015.01\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————测试：建立101层的残差网络————————————————————————————\n",
    "res101 = ResNet(Bottleneck, layers=[3,4,23,3], num_classes=1000)\n",
    "\n",
    "summary(res101,datashape,depth=2,device=\"cpu\")\n",
    "\n",
    "\n",
    "# 101层只有7kw+个参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里，我们就已经复现了整个残差网络。即便ResNet现在可以达到的深度非常深，但从参数和计算量的角度来看，它并不算是“巨型”的模型。\n",
    "\n",
    "从模型效果来看，残差网络毫无疑问是现有的最顶尖的模型之一，几乎所有大型数据集的跑分榜单前几名都是残差网络占据。除了我们已经学习的基本网络，残差网络还有许多有效、强大的变体，如ResNeXt（在残差网络上加入了并联结构），WideResNet（目前为止最强大的模型）等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3296471c9107718e6db2104fe506aa09d2e74f3fa2347eed57e489ba84541490"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
