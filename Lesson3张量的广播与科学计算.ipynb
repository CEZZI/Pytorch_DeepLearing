{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3.张量的广播和科学运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **数学运算与算子**\n",
    "\n",
    "&emsp;&emsp;在PyTorch中，**能够作用与Tensor的运算，被统一称作为算子**。\n",
    "\n",
    "- **数学运算的分类  **    \n",
    " PyToch总共为Tensor设计了六大类数学运算，分别是：      \n",
    "  - 1.逐点运算（Pointwise Ops）：指的是针对Tensor中每个元素执行的相同运算操作；      \n",
    "  - 2.规约运算（Reduction Ops）：指的是对于某一张量进行操作得出某种总结值；      \n",
    "  - 3.比较运算（Comparison Ops）：指的是对多个张量进行比较运算的相关方法；      \n",
    "  - 4.谱运算（Spectral Ops）：指的是涉及信号处理傅里叶变化的操作；        \n",
    "  - 5.BLAS和LAPACK运算：指的是基础线性代数程序集（Basic Linear Algeria Subprograms）和线性代数包（Linear Algeria Package）中定义的、主要用于线性代数科学计算的函数和方法；      \n",
    "  - 6.其他运算（Other Ops）：其他未被归类的数学运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、张量的广播（Broadcast）特性\n",
    "\n",
    "### 1.相同形状的张量计算\n",
    "\n",
    "相同形状数组总是可以进行广播计算,对应位置元素进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t1  #  逐点运算 对应元素的相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.不同形状的张量计算\n",
    "\n",
    "&emsp;&emsp;广播的特性是在**不同形状的张量**进行计算时，一个或多个张量通过隐式转化，转化成相同形状的两个张量，从而完成计算的特性。**但并非任何两个不同形状的张量都可以通过广播特性进行计算**，因此，我们需要了解广播的基本规则及其核心依据。\n",
    "\n",
    "#### 2.1 变量和任意形状的张量\n",
    "\n",
    "&emsp;&emsp;标量可以和任意形状的张量进行计算，计算过程就是标量和张量的每一个元素进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + 1  # 1是标量（可以看做0维度） 对张量中的每一元素都会相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二维张量+0维\n",
    "t1 + torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.zeros((3,4))\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 相同维度、不同形状的张量之间计算\n",
    "\n",
    "&emsp;&emsp;对于不同形状的张量计算，我们首先需要回顾张量的形状属性，并深化对其的理解。\n",
    "\n",
    "&emsp;&emsp;首先，我们都知道，张量的形状可以用.shape属性查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于返回结果，我们可以看成是一个序列，代表着张量各维度的信息。当然，对于二维张量，由于我们可以将其视作一个矩阵，因此我们可以说t2是一个拥有三行四列的二维张量，但这种理解方式对于更高维度张量就存在一定的局限，因此我们需要树立另外一种理解方法，那就是：t2是由3个一维张量组成，并且该一维张量、每个都包含四个元素。类似的，我们可以创建更高维度张量并对其形状进行解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.zeros(3,4,5)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将t3解释为：t3是3个二维张量组成了三维张量，并且这些每个二维张量，都是由四个包含五个元素的一维张量所组成。由二维拓展至三维，即可拓展至N维。\n",
    "\n",
    "接下来，我们以t2为例，来探讨相同维度、不同形状的张量之间的广播规则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t21 = torch.ones(1,4)\n",
    "t21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t21的形状是（1， 4），和t2的形状（3， 4）在第一个分量上取值不同，但该分量上t21取值为1，因此可以广播，也就可以进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 + t21\n",
    "\n",
    "# 相当于t21 复制了3次 加到了t2的每一行去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t22 = torch.ones(3,1)\n",
    "t22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 + t22\n",
    "# 把t22的一列 复制了3次 加到了t2的每一列中去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/13/Y7bd8sNRO4iC5mZ.jpg\" alt=\"2\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t23 = torch.ones(2,4)\n",
    "t23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-8e76d68d3280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt23\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 注：此时t2和t23的形状第一个分量维度不同，\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 但二者取值均不为1，因此无法广播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "t2 + t23\n",
    "\n",
    "# 注：此时t2和t23的形状第一个分量维度不同，\n",
    "# 但二者取值均不为1，因此无法广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t24 = torch.arange(3).reshape(3, 1)\n",
    "t24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 + t24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t25 = torch.arange(3).reshape(1, 3)\n",
    "t25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [1, 2, 3],\n",
       "        [2, 3, 4]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t24 +t25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，t24的形状是（3， 1），而t25的形状是（1， 3），二者的形状在两个份量上均不相同，但都有存在1的情况，因此也是可以广播的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4](https://i.loli.net/2021/01/13/rX53dcTDMBxGmg4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三维张量的广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.zeros(3, 4, 5)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t31 = torch.ones(3, 4, 1)\n",
    "t31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 + t31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t32 = torch.ones(3, 1, 5)\n",
    "t32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t32 + t3\n",
    "\n",
    "# t32中一行的复制4边 加到t3上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个张量的形状上有两个分量不同时，只要不同的分量仍然有一个取值为1，则仍然可以广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t33 = torch.ones(1, 1, 5)\n",
    "t33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 + t33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t3和t33计算过程如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/13/eFR3UnsEwz1v4Ax.jpg\" alt=\"3\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：此处标注的两次广播，我们也可认为上述全部过程的实现是一次“大的”广播。同时，此处最开始的t33也就相当于一个一维的、包含五个元素的张量，因此上述过程也可视为一个一维张量和一个三维张量计算时的广播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 不同维度的张量计算过程中广播\n",
    "\n",
    "&emsp;&emsp;因为对于不同维度的张量，我们首先可以将低维的张量升维，然后依据相同维度不同形状的张量广播规则进行广播。而低维向量的升维也非常简单，只需将更高维度方向的形状填充为1即可，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二维张量\n",
    "t2 = torch.arange(4).reshape(2, 2)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为三维张量\n",
    "t2.reshape(1, 2, 2)\n",
    "\n",
    "# 转化之后表示只包含一个二维张量的三维张量，且二维张量就是t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1],\n",
       "          [2, 3]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为四维张量\n",
    "t2.reshape(1, 1, 2, 2)\n",
    "\n",
    "# 转化之后表示只包含一个三维张量的四维张量，且三维张量只包含一个二维张量，且二维张量就是t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.zeros(3, 2, 2)\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t3和t2的计算过程，就相当于形状为（1，2，2）和（3，2，2）的两个张量进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 + t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[0., 1.],\n",
       "         [2., 3.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 + t2.reshape(1, 2, 2)  # t2 升维  可以直接相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、逐点运算（Pointwise Ops）\n",
    "\n",
    "对Tensor中每个元素都进行的数学科学运算，并且都是较为通用的数学科学运算。\n",
    "\n",
    "逐点运算主要包括数学基本运算、数值调整运算和数据科学运算三块，相关函数如下：\n",
    "\n",
    "**<center>Tensor基本数学运算</center>**\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| `torch.add(t1，t2 )`      | t1、t2两个张量逐个元素相加，等效于t1+t2     |\n",
    "| `torch.subtract(t1，t2)` | t1、t2两个张量逐个元素相减，等效于t1-t2      |\n",
    "|` torch.multiply(t1，t2)` | t1、t2两个张量逐个元素相乘，等效于t1\\*t2            |\n",
    "| `torch.divide(t1，t2)`   | t1、t2两个张量逐个元素相除，等效于t1/t2            |\n",
    "\n",
    "**<center>Tensor数值调整函数</center>**\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.abs(t)        | 返回绝对值 | \n",
    "| torch.ceil(t)       | 向上取整 | \n",
    "| torch.floor(t)      | 向下取整 |\n",
    "| torch.round(t)      | 四舍五入取整 |\n",
    "| torch.neg(t)      | 返回相反的数 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0613, -0.2962, -0.5755, -1.5193,  1.0180])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(5)  #  服从标准正态分布的5个个数\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -0., -1., -2.,  1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而若要对原对象本身进行修改，则可考虑使用`方法_()`的表达形式，对对象本身进行修改。此时方法就是上述同名函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0613, 0.2962, 0.5755, 1.5193, 1.0180])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.abs_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0613, 0.2962, 0.5755, 1.5193, 1.0180])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t  #  t发生了改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0613, -0.2962, -0.5755, -1.5193, -1.0180])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0613, -0.2962, -0.5755, -1.5193, -1.0180])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>Tensor常用科学计算</center>**\n",
    "\n",
    "|**数学运算函数**|**数学公式**|**描述**|\n",
    "| :------:| :------: | :------: |\n",
    "| 幂运算 |\n",
    "| torch.exp(t)        |$ y_{i} = e^{x_{i}} $ | 返回以e为底、t中元素为幂的张量 | \n",
    "| torch.expm1(t)         | $ y_{i} = e^{x_{i}} $ - 1 |对张量中的所有元素计算exp（x） - 1|\n",
    "| torch.exp2(t)          | $ y_{i} = 2^{x_{i}} $ |逐个元素计算2的t次方。 | \n",
    "| torch.pow(t,n)       | $\\text{out}_i = x_i ^ \\text{exponent} $ | 返回t的n次幂 | \n",
    "| torch.sqrt(t)       |$ \\text{out}_{i} = \\sqrt{\\text{input}_{i}} $ | 返回t的平方根 | \n",
    "| torch.square(t)        |$ \\text{out}_i = x_i ^ \\text{2} $ | 返回输入的元素平方。                     | \n",
    "| 对数运算 |\n",
    "| torch.log10(t)      |$ y_{i} = \\log_{10} (x_{i}) $ | 返回以10为底的t的对数 | \n",
    "| torch.log(t)  |$ y_{i} = \\log_{e} (x_{i}) $| 返回以e为底的t的对数 |\n",
    "| torch.log2(t)          |$ y_{i} = \\log_{2} (x_{i}) $| 返回以2为底的t的对数                         | \n",
    "| torch.log1p(t)         |$ y_i = \\log_{e} (x_i $ + 1)| 返回一个加自然对数的输入数组。     | \n",
    "| 三角函数运算|\n",
    "| torch.sin(t)           |三角正弦。                               | \n",
    "| torch.cos(t)           | 元素余弦。                               | \n",
    "| torch.tan(t)           |逐元素计算切线。                         | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor的大多数科学计算只能作用于tensor对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pow() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor input, Tensor exponent, *, Tensor out)\n * (Number self, Tensor exponent, *, Tensor out)\n * (Tensor input, Number exponent, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-89fb35bb773f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 计算2的2次方\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: pow() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor input, Tensor exponent, *, Tensor out)\n * (Number self, Tensor exponent, *, Tensor out)\n * (Tensor input, Number exponent, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "# 计算2的2次方\n",
    "torch.pow(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(torch.tensor(2),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解：相比于Python原生数据类型，张量是一类更加特殊的对象，例如张量可以指定运行在CPU或者GPU上，因此很多张量的科学计算函数都不允许张量和Python原生的数值型对象混合使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor的大多数科学运算具有一定的静态性\n",
    "\n",
    "  所谓静态性，指的是对输入的张量类型有明确的要求，例如部分函数只能输入浮点型张量，而不能输入整型张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1,4) \n",
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7183,  7.3891, 20.0855])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(t)  # t是整数型的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = t.float()\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7183,  7.3891, 20.0855])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7183,  6.3891, 19.0855])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.expm1(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注，此处返回结果是$e^{t} - 1$，在数值科学计算中，expm1函数和log1p函数是一对对应的函数关系，后面再介绍log1p的时候会讲解这对函数的实际作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 8.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp2(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(t, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意区分 2的t次方和t的2次方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，我们也可简单回顾幂运算和对数运算之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.log(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp2(torch.log2(t1))  #以2为底"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 排序运算：sort\n",
    "\n",
    "&emsp;&emsp;在PyTorch中，sort排序函数将**同时返回排序结果和对应的索引值的排列**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 2.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1.0, 3.0, 2.0])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([1., 2., 3.]),\n",
       "indices=tensor([0, 2, 1]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 升序排列\n",
    "torch.sort(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([3., 2., 1.]),\n",
       "indices=tensor([1, 2, 0]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 降序排列\n",
    "torch.sort(t, descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、规约运算\n",
    "\n",
    "规约运算指的是针对某张量进行某种总结，最后得出一个具体总结值的函数。此类函数主要包含了数据科学领域内的诸多统计分析函数，如均**值、极值、方差、中位数**函数等等。\n",
    "\n",
    "**<center>Tensor统计分析函数</center>**\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.mean(t)        | 返回张量均值 | \n",
    "| torch.var(t)       | 返回张量方差 | \n",
    "| torch.std(t)        | 返回张量标准差 | \n",
    "| torch.var_mean(t)       | 返回张量方差和均值 | \n",
    "| torch.std_mean(t)       | 返回张量标准差和均值 | \n",
    "| torch.max(t)        | 返回张量最大值 | \n",
    "| torch.argmax(t)        | 返回张量最大值索引 | \n",
    "| torch.min(t)       | 返回张量最小值 | \n",
    "| torch.argmin(t)       | 返回张量最小值索引 | \n",
    "| torch.median(t)        | 返回张量中位数 | \n",
    "| torch.sum(t)       | 返回张量求和结果 | \n",
    "| torch.logsumexp(t)       | 返回张量各元素求和结果，适用于数据量较小的情况 | \n",
    "| torch.prod(t)        | 返回张量累乘结果 | \n",
    "| torch.dist(t1, t2)        | 计算两个张量的闵式距离，可使用不同范式 |\n",
    "| torch.topk(t)        | 返回t中最大的k个值对应的指标 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成浮点型张量\n",
    "t = torch.arange(10).float()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5000)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.0277), tensor(4.5000))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std_mean(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回最大值的索引\n",
    "torch.argmax(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dist计算距离\n",
    "\n",
    "&emsp;&emsp;dist函数可计算闵式距离（闵可夫斯基距离），通过输入不同的p值，可以计算多种类型的距离，如欧式距离、街道距离等。闵可夫斯基距离公式如下：      \n",
    "$$ D(x,y) = (\\sum^{n}_{u=1}|x_u-y_u|^{p})^{1/p}$$\n",
    "\n",
    "- p取值为2时，计算欧式距离\n",
    "- p取值为1时，计算街道距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1.0, 2])\n",
    "t2 = torch.tensor([3.0, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8284)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(t1,t2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(t1,t2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 规约运算的维度\n",
    "\n",
    "&emsp;&emsp;由于规约运算是一个序列返回一个结果，因此若是针对高维张量，则可指定某维度进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).float().reshape(3,4)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 15., 18., 21.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照第一个维度(行 垂直方向)求和（每次计算三个）、按列求和\n",
    "torch.sum(t2, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 22., 38.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按照第二个维度求和（每次计算四个）、按行求和\n",
    "torch.sum(t2, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个2*3*4的三维张量\n",
    "t3 = torch.arange(24).float().reshape(2, 3, 4)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 14., 16., 18.],\n",
       "        [20., 22., 24., 26.],\n",
       "        [28., 30., 32., 34.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t3, dim = 0)   \n",
    "# 在第一维上相加 两个矩阵相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 15., 18., 21.],\n",
       "        [48., 51., 54., 57.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t3, dim = 1)  \n",
    "# 在第二维上操作 得到结果为每3个元素的相加即一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6., 22., 38.],\n",
       "        [54., 70., 86.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t3, dim = 2)\n",
    "# 在第三维上操作，即每4个元素相加可得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二维张量的排序\n",
    "\n",
    "和上述过程类似，在进行排序过程中，二维张量也可以按行或者按列进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2449, -0.4393,  0.9118,  1.5717],\n",
       "        [-2.2552, -0.5566, -1.7134,  0.4670],\n",
       "        [ 1.2737, -2.4496, -0.8531, -1.4028]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t22 = torch.randn(3, 4)             # 创建二维随机数张量\n",
    "t22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[-0.4393,  0.2449,  0.9118,  1.5717],\n",
       "        [-2.2552, -1.7134, -0.5566,  0.4670],\n",
       "        [-2.4496, -1.4028, -0.8531,  1.2737]]),\n",
       "indices=tensor([[1, 0, 2, 3],\n",
       "        [0, 2, 1, 3],\n",
       "        [1, 3, 2, 0]]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下，是按照行（在一行里进行排序）进行升序排序\n",
    "torch.sort(t22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[ 1.5717,  0.9118,  0.2449, -0.4393],\n",
       "        [ 0.4670, -0.5566, -1.7134, -2.2552],\n",
       "        [ 1.2737, -0.8531, -1.4028, -2.4496]]),\n",
       "indices=tensor([[3, 2, 0, 1],\n",
       "        [3, 1, 2, 0],\n",
       "        [0, 2, 3, 1]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 修改dim和descending参数，使得按列进行降序排序\n",
    "torch.sort(t22, dim = 1, descending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、比较运算\n",
    "\n",
    "和Python原生的布尔运算类似，常用于不同张量之间的逻辑运算，最终返回逻辑运算结果\n",
    "\n",
    "**<center>Tensor比较运算函数</center>**\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.eq(t1, t2)        | 比较t1、t2各元素是否相等，等效==| \n",
    "| torch.equal(t1, t2)       | 判断两个张量是否是相同的张量 | \n",
    "| torch.gt(t1, t2)        | 比较t1各元素是否大于t2各元素，等效>| \n",
    "| torch.lt(t1, t2)        | 比较t1各元素是否小于t2各元素，等效<| \n",
    "| torch.ge(t1, t2)        | 比较t1各元素是否大于或等于t2各元素，等效>=| \n",
    "| torch.le(t1, t2)        | 比较t1各元素是否小于等于t2各元素，等效<=| \n",
    "| torch.ne(t1, t2)        | 比较t1、t2各元素是否不相同，等效!=| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1.0, 3, 4])\n",
    "t2 = torch.tensor([1.0, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(t1, t2)          # 判断t1、t2是否是相同的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 > t2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9c95e2fca121e63b81ef3e38ca658e3e94770f8b66e1c1e59a7460ee39b328d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
