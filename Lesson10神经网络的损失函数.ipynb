{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10 神经网络的损失函数\n",
    "\n",
    "我们将从梯度下降法向外拓展，介绍神经网络的损失函数、常用优化算法等信息，实现神经网络的学习和迭代。本节主要讲解神经网络常用的损失函数，并在PyTorch中实现这些函数。\n",
    "\n",
    "\n",
    "##  一、机器学习中的优化思想\n",
    "\n",
    "我们建立神经网络时总是先设定好$w$与$b$的值（或者由我们调用的PyTorch类帮助我们随机生成权重向量$w$），接着通过加和求出$z$，再在$z$上嵌套sigmoid或者softmax函数，最终获得神经网络的输出。我们的代码及计算流程，总是从神经网络的左侧向右侧计算的。\n",
    "\n",
    "线性回归的任务就是构造一个预测函数来映射输入的特征矩阵$X$和标签值$y$线性关系。构造预测函数核心就是找出模型的权重向量$w$，并令线性回归的输出结果与真实值相近，也就是求解线性方程组中的$w$和$b$。对神经网络而言也是如此，我们的核心任务是求解一组最适合的$w$和$b$，**令神经网络的输出结果与真实值接近**。\n",
    "\n",
    "**优化流程**\n",
    "1. 提出基本模型，明确目标\n",
    "2. 确定损失函数/目标函数\n",
    "   - 转化为凸函数\n",
    "   - 在凸函数上，求解对应L(w)最小值的w\n",
    "3. 确定适合的优化算法\n",
    "4. 确定优化算法，最小化损失函数，求解最佳权重\n",
    "\n",
    "**损失函数**  \n",
    "衡量真实值与预测结果的差异，评价模型学习过程中产生的损失的函数,以需要求解的权重向量$w$为自变量的函数$L(w)$ 。\n",
    "  - 损失函数的值很小，则说明模型预测值与真实值很接近，模型在数据集上表现优异，权重优秀\n",
    "  - 损失函数的值大，则说明模型预测值与真实值差异很大，模型在数据集上表现差劲，权重糟糕"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、回归: 误差平方和SSE\n",
    "\n",
    "$$SSE= \\sum_{i=1}^m{(z_i-\\hat{z_i})^2}$$\n",
    "\n",
    "- 均方误差：全部样本的平均损失：\n",
    "\n",
    "$$MSE= \\frac{1}{m}\\sum_{i=1}^m{(z_i-\\hat{z_i})^2}$$\n",
    "\n",
    "\n",
    "- `criterion = MSELoss();criterion(预测，真实值)` 实例化MSELoss类\n",
    "  - 参数：默认reduction = \"mean\"，返回均方误差\n",
    "  - reduction = \"sum\",返回总体的误差平方和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss  # MSELoss是一个类，调用需要实例化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机生成数据\n",
    "yhat = torch.randn(size=(50,),dtype=torch.float32)\n",
    "y = torch.randn(size=(50,),dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3172)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化\n",
    "criterion = MSELoss()\n",
    "loss = criterion(yhat,y)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3172)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————均方误差———————————————————————————\n",
    "\n",
    "criterion = MSELoss(reduction = \"mean\") #实例化\n",
    "criterion(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65.8581)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————总体平方误差————————————————————————————\n",
    "criterion = MSELoss(reduction = \"sum\")\n",
    "criterion(yhat,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、二分类交叉熵损失函数\n",
    "\n",
    "我们将介绍二分类神经网络的损失函数：二分类交叉熵损失函数，也叫做对数损失。这个损失函数被广泛地使用在任何输出结果是二分类的神经网络中，即不止限于单层神经网络，还可被拓展到多分类中。\n",
    "\n",
    "二分类交叉熵损失函数是由极大似然估计推导出来的，对于有m个样本的数据集而言，在全部样本上的平均损失写作：\n",
    "$$L(w)=-\\sum_{i=1}^m{(y_i*\\ln{\\sigma_i}+(1-y_i)*\\ln{(1-\\sigma_i}))}$$\n",
    "\n",
    "- $\\sigma_i$即在第1类上的概率\n",
    "- 本来极大似然求得最大值，加-号变成求解损失函数的最小值\n",
    "- 这个公式很重要！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.极大似然估计求解二分类交叉熵损失\n",
    "\n",
    "**极大似然估计**  \n",
    "&emsp;&emsp;如果一个事件的发生概率很大，那这个事件应该很容易发生。相应的，如果依赖于权重 的任意事件的发生就是我们的目标，那我们只要寻找令其发生概率最大化的权重$w$就可以了。**寻找相应的权重$w$，使得目标事件的发生概率最大**.\n",
    "其步骤如下：  \n",
    "1. 构筑似然函数$P(w)$，用于评估目标事件发生的概率，该函数被设计成目标事件发生时，概率最大  \n",
    "2. 对整体似然函数取对数，构成对数似然函数$\\ln{P(w)}$\n",
    "3. 在对数似然函数上对权重 求导，并使导数为0，对权重进行求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.用tensor实现二分类交叉熵损失\n",
    "\n",
    "- 尽量使用torch中的运算符，运算更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = -(y*ln(sigma)+(1-y)*ln(1-sigma))\n",
    "# y 真实标签\n",
    "# z = Xw\n",
    "# X,w\n",
    "# m个样本\n",
    "\n",
    "N = 3 * pow(10,3)  # 3000个数据\n",
    "torch.random.manual_seed(420)\n",
    "X = torch.rand((N,4),dtype=torch.float32) # 4个特征\n",
    "w = torch.rand((4,1),dtype=torch.float32,requires_grad=True)\n",
    "y = torch.randint(low=0,high=2,size=(N,1),dtype=torch.float32)   # 设置二分类的标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w是二维的，就是用torch.mm\n",
    "zhat = torch.mm(X,w)\n",
    "sigma = torch.sigmoid(zhat)\n",
    "sigma.shape  # 返回3000个样本的概率 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3075],\n",
       "        [0.3073],\n",
       "        [0.9198],\n",
       "        ...,\n",
       "        [0.3876],\n",
       "        [0.4536],\n",
       "        [0.3442]], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss = -(y*torch.log(sigma)+(1-y)*torch.log(1-sigma))  # 计算的是每个样本单独的损失 \n",
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7962], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回均方误差\n",
    "loss = 1/N*(-sum((y*torch.log(sigma)+(1-y)*torch.log(1-sigma))))\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07133221626281738\n",
      "72.7374906539917\n"
     ]
    }
   ],
   "source": [
    "#————————————————————————————尽量使用Torch中的运算————————————————————————————\n",
    "#你可以试着比较在样本量为300W时，以下两行代码运行的时间差异。这段代码不需要GPU。\n",
    "#如果你的电脑内存或计算资源有限，可以试着将样本量调小为30W或3W\n",
    "\n",
    "N = 3*pow(10,6)\n",
    "torch.random.manual_seed(420)\n",
    "X = torch.rand((N,4),dtype=torch.float32)\n",
    "w = torch.rand((4,1),dtype=torch.float32,requires_grad=True)\n",
    "y = torch.randint(low=0,high=2,size=(N,1),dtype=torch.float32)\n",
    "\n",
    "zhat = torch.mm(X,w)\n",
    "sigma = torch.sigmoid(zhat)\n",
    "\n",
    "# torch.sum() 用了0.07秒\n",
    "start = time.time()\n",
    "L1 = -(1/N)*torch.sum((1-y)*torch.log(1-sigma)+y*torch.log(sigma))\n",
    "now = time.time() #seconds\n",
    "print(now - start)\n",
    "\n",
    "\n",
    "# 计算的很慢...72秒\n",
    "start = time.time()\n",
    "L2 = -(1/N)*sum((1-y)*torch.log(1-sigma)+y*torch.log(sigma))\n",
    "now = time.time() #seconds\n",
    "print(now - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从运行结果来看，除了加减乘除，我们应该尽量避免使用任何Python原生的计算方法。如果可能的话，让PyTorch处理一切。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.用PyTorch中的类实现二分类交叉熵损失\n",
    "\n",
    "**方法一：nn模块中的类**\n",
    "- `class` BCEWithLogitsLoss \n",
    "  - 内置了sigmoid函数与交叉熵函数，它会自动计算输入值的sigmoid值\n",
    "  - 因此需要输入**zhat与真实标签**，且顺序不能变化，zhat必须在前\n",
    "  - 数据量较大时，计算sigmoid函数，可能有精度问题\n",
    "- `class` BCELoss\n",
    "  - 只有交叉熵函数，没有sigmoid层\n",
    "  - 因此需要输入sigma与真实标签，且顺序不能变化\n",
    "  - 需要监控准确率时使用\n",
    "- 二分类交叉熵的类们也有`参数reduction`，\n",
    "  - 默认是mean\n",
    "  - sum 要求输出整体的损失\n",
    "  - none 示不对损失结果做任何聚合运算，直接输出每个样本对应的损失矩阵\n",
    "- 两个函数都要求预测值与真实标签的数据类型以及结构（shape）必须相同，否则运行就会报错。\n",
    "\n",
    "**方法二：functional库中的计算函数**  \n",
    "- 两个不常用的函数\n",
    "- `function` F.binary_cross_entropy_with_logits\n",
    "- `function` F.binary_cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7962, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————使用类：nn.BCELoss() ————————————————————————————\n",
    "# 实例化（使用3000个数据的sigma）\n",
    "criterion = nn.BCELoss() \n",
    "loss = criterion(sigma,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7962, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————使用类：nn.BCEWithLogitsLoss()————————————————————————————\n",
    "\n",
    "criterion2 = nn.BCEWithLogitsLoss()  # 实例化\n",
    "loss2 = criterion2(zhat,y)\n",
    "loss2\n",
    "\n",
    "#内置的sigmoid函数可以让精度问题被缩小（因为将指数运算包含在了内部），以维持算法运行时的稳定性，\n",
    "# 即是说当数据量变大、数据本身也变大时，BCELoss类产生的结果可能有精度问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7962, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion2 = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "loss = criterion2(zhat,y)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2388.5840, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion2 = nn.BCEWithLogitsLoss(reduction = \"sum\")\n",
    "loss = criterion2(zhat,y)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3075],\n",
       "        [0.3073],\n",
       "        [0.9198],\n",
       "        ...,\n",
       "        [0.3876],\n",
       "        [0.4536],\n",
       "        [0.3442]], grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion2 = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "loss = criterion2(zhat,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7962, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————方法二：使用函数————————————————————————————\n",
    "from torch.nn import functional as F\n",
    "#直接调用functional库中的计算函数\n",
    "F.binary_cross_entropy_with_logits(zhat,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7962, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(sigma,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、多分类交叉熵损失函数 \n",
    "\n",
    "#### 1.由二分类推广到多分类\n",
    "\n",
    "多分类问题中，不服从伯努利分布，样本标签被预测为k的概率为：\n",
    "$$P_k=P(\\hat{y}_i=k|x_i,w)=\\sigma$$\n",
    "在多分类问题中，$\\sigma$就是softmax对于类别返回的值\n",
    "- 对原来的真实标签[1,2,3],做独热编码[1,0,0],[0,1,0],[0,0,1]\n",
    "- 交叉熵函数$$L(w)=-\\sum_{i=1}^m{y_i(k=j)\\ln{\\sigma_i}}$$\n",
    "  - 我们把$\\ln{softmax(z)}$这样的函数单独定义了一个功能做logsoftmax，PyTorch中可以直接通过nn.logsoftmax类调用\n",
    "  - 同时，我们把对数之外的，乘以标签、加和、取负等等过程打包起来，称之为负对数似然函数，在PyTorch中可以使用nn.NLLLoss来进行调用。\n",
    "  - 也就是说，在计算损失函数时，我们不再需要使用单独的softmax函数了。\n",
    "\n",
    "#### 2.用Pytorch实现多分类交叉熵损失\n",
    "\n",
    "- 调用logsoftmax和MLLLoss实现\n",
    "- 直接调用CrossEntropyLoss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "N = 3*pow(10,2)\n",
    "torch.random.manual_seed(420)\n",
    "X = torch.rand((N,4),dtype=torch.float32)\n",
    "w = torch.rand((4,3),dtype=torch.float32,requires_grad=True)\n",
    "#定义y时应该怎么做？应该设置为矩阵吗？\n",
    "y = torch.randint(low=0,high=3,size=(N,),dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhat = torch.mm(X,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1591, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#————————————————————————————里开始调用softmax和NLLLoss————————————————————————————\n",
    "logsm = nn.LogSoftmax(dim=1)  # 实例化\n",
    "logsigma = logsm(zhat)\n",
    "\n",
    "criterion = nn.NLLLoss() # 实例化\n",
    "criterion(logsigma,y.long())\n",
    "\n",
    "# 必须把y转化为整形long\n",
    "\n",
    "# 可以发现，两种输出方法得到的损失函数结果是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 直接调用CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1591, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_ = nn.CrossEntropyLoss() \n",
    "#对打包好的CorssEnrtopyLoss而言，只需要输入zhat\n",
    "criterion_(zhat,y.long())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9c95e2fca121e63b81ef3e38ca658e3e94770f8b66e1c1e59a7460ee39b328d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
